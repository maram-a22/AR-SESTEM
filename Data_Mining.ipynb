{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/maram-a22/IR-SESTEM/blob/main/Data_Mining.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "736AMDmKPkC5",
        "outputId": "c142ac96-ca62-42b4-9890-aef2d7293562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install  nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DjA1bo1EWZZH",
        "outputId": "b66f7be7-e101-41fa-c270-b1e1108ae8c2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import string\n",
        "string.punctuation\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('wordnet')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuTST0eMTq0V",
        "outputId": "2dbfc5fc-74aa-45bc-c3da-60800efe0707"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_uq7hdyAV62P",
        "outputId": "dcf5380d-b046-437d-8e96-fd408256f816"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import nltk.data\n",
        "import string\n",
        "import re\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "wnl = WordNetLemmatizer()\n",
        "from string import digits\n",
        "\n",
        "def preprocessing(f):\n",
        "  #Tokenization\n",
        " with f as file:\n",
        "  content=file.read()\n",
        "  nltk_words = word_tokenize(content)\n",
        "  #print('after token')\n",
        "  #print(nltk_words)\n",
        "\n",
        "  # Lower Casing - Stop word removal - remove numbers -remove punctuation\n",
        "  punctuation = list(string.punctuation)\n",
        "  stopWords = set(stopwords.words('english'))\n",
        "  filter = []\n",
        "  for w in nltk_words:\n",
        "    if w.lower()not in stopWords  :\n",
        "        w=re.sub(r'[0-9]+', '', w)\n",
        "        if w not in punctuation and w != '':\n",
        "          filter.append(w.lower())\n",
        "  #print('after stopword removed----------------------------------------------')\n",
        "  #print('-------------------------------------------------------------------')\n",
        "  #print(filter)\n",
        "\n",
        "  #print('stemming-----------------------------------------------------------')\n",
        "  #print('-------------------------------------------------------------------')\n",
        "  #Stemming\n",
        "  ps = PorterStemmer()\n",
        "  words= filter\n",
        "  for w in words:\n",
        "   # print(w, \" : \", ps.stem(w))\n",
        "\n",
        "  #print()\n",
        "  #print('lemmatize--------------------------------------------------')\n",
        "  #print('-----------------------------------------------------------')\n",
        "  #Lemmatization\n",
        "  for w in words:\n",
        "    print(w + \" ---> \" + wnl.lemmatize(w))\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "moSa6QMjWvep",
        "outputId": "190632c3-3e2a-439b-f9bb-2c1edbda6bca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "after token\n",
            "['While', 'a', 'number', 'of', 'definitions', 'of', 'artificial', 'intelligence', '(', 'AI', ')', 'have', 'surfaced', 'over', 'the', 'last', 'few', 'decades', ',', 'John', 'McCarthy', 'offers', 'the', 'following', 'definition', 'in', 'this', '2004', 'paper', '(', 'PDF', ',', '106', 'KB', ')', '(', 'link', 'resides', 'outside', 'IBM', ')', ',', 'It', 'is', 'the', 'science', 'and', 'engineering', 'of', 'making', 'intelligent', 'machines', ',', 'especially', 'intelligent', 'computer', 'programs', '.', 'It', 'is', 'related', 'to', 'the', 'similar', 'task', 'of', 'using', 'computers', 'to', 'understand', 'human', 'intelligence', ',', 'but', 'AI', 'does', 'not', 'have', 'to', 'confine', 'itself', 'to', 'methods', 'that', 'are', 'biologically', 'observable', '.', 'However', ',', 'decades', 'before', 'this', 'definition', ',', 'the', 'birth', 'of', 'the', 'artificial', 'intelligence', 'conversation', 'was', 'denoted', 'by', 'Alan', 'Turing', 'seminal', 'work', ',', 'Computing', 'Machinery', 'and', 'Intelligence', '(', 'PDF', ',', '89.8', 'KB', ')', '(', 'link', 'resides', 'outside', 'of', 'IBM', ')', ',', 'which', 'was', 'published', 'in', '1950', '.', 'In', 'this', 'paper', ',', 'Turing', ',', 'often', 'referred', 'to', 'as', 'the', 'father', 'of', 'computer', 'science', ',', 'asks', 'the', 'following', 'question', ',', 'Can', 'machines', 'think', '?', 'From', 'there', ',', 'he', 'offers', 'a', 'test', ',', 'now', 'famously', 'known', 'as', 'the', 'Turing', 'Test', ',', 'where', 'a', 'human', 'interrogator', 'would', 'try', 'to', 'distinguish', 'between', 'a', 'computer', 'and', 'human', 'text', 'response', '.', 'While', 'this', 'test', 'has', 'undergone', 'much', 'scrutiny', 'since', 'its', 'publish', ',', 'it', 'remains', 'an', 'important', 'part', 'of', 'the', 'history', 'of', 'AI', 'as', 'well', 'as', 'an', 'ongoing', 'concept', 'within', 'philosophy', 'as', 'it', 'utilizes', 'ideas', 'around', 'linguistics', '.', 'Stuart', 'Russell', 'and', 'Peter', 'Norvig', 'then', 'proceeded', 'to', 'publish', ',', 'Artificial', 'Intelligence', ':', 'A', 'Modern', 'Approach', '(', 'link', 'resides', 'outside', 'IBM', ')', ',', 'becoming', 'one', 'of', 'the', 'leading', 'textbooks', 'in', 'the', 'study', 'of', 'AI', '.', 'In', 'it', ',', 'they', 'delve', 'into', 'four', 'potential', 'goals', 'or', 'definitions', 'of', 'AI', ',', 'which', 'differentiates', 'computer', 'systems', 'on', 'the', 'basis', 'of', 'rationality', 'and', 'thinking', 'vs.', 'acting', ':']\n",
            "after stopword removed----------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "['number', 'definitions', 'artificial', 'intelligence', 'ai', 'surfaced', 'last', 'decades', 'john', 'mccarthy', 'offers', 'following', 'definition', 'paper', 'pdf', 'kb', 'link', 'resides', 'outside', 'ibm', 'science', 'engineering', 'making', 'intelligent', 'machines', 'especially', 'intelligent', 'computer', 'programs', 'related', 'similar', 'task', 'using', 'computers', 'understand', 'human', 'intelligence', 'ai', 'confine', 'methods', 'biologically', 'observable', 'however', 'decades', 'definition', 'birth', 'artificial', 'intelligence', 'conversation', 'denoted', 'alan', 'turing', 'seminal', 'work', 'computing', 'machinery', 'intelligence', 'pdf', 'kb', 'link', 'resides', 'outside', 'ibm', 'published', 'paper', 'turing', 'often', 'referred', 'father', 'computer', 'science', 'asks', 'following', 'question', 'machines', 'think', 'offers', 'test', 'famously', 'known', 'turing', 'test', 'human', 'interrogator', 'would', 'try', 'distinguish', 'computer', 'human', 'text', 'response', 'test', 'undergone', 'much', 'scrutiny', 'since', 'publish', 'remains', 'important', 'part', 'history', 'ai', 'well', 'ongoing', 'concept', 'within', 'philosophy', 'utilizes', 'ideas', 'around', 'linguistics', 'stuart', 'russell', 'peter', 'norvig', 'proceeded', 'publish', 'artificial', 'intelligence', 'modern', 'approach', 'link', 'resides', 'outside', 'ibm', 'becoming', 'one', 'leading', 'textbooks', 'study', 'ai', 'delve', 'four', 'potential', 'goals', 'definitions', 'ai', 'differentiates', 'computer', 'systems', 'basis', 'rationality', 'thinking', 'vs.', 'acting']\n",
            "stemming-----------------------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "number  :  number\n",
            "definitions  :  definit\n",
            "artificial  :  artifici\n",
            "intelligence  :  intellig\n",
            "ai  :  ai\n",
            "surfaced  :  surfac\n",
            "last  :  last\n",
            "decades  :  decad\n",
            "john  :  john\n",
            "mccarthy  :  mccarthi\n",
            "offers  :  offer\n",
            "following  :  follow\n",
            "definition  :  definit\n",
            "paper  :  paper\n",
            "pdf  :  pdf\n",
            "kb  :  kb\n",
            "link  :  link\n",
            "resides  :  resid\n",
            "outside  :  outsid\n",
            "ibm  :  ibm\n",
            "science  :  scienc\n",
            "engineering  :  engin\n",
            "making  :  make\n",
            "intelligent  :  intellig\n",
            "machines  :  machin\n",
            "especially  :  especi\n",
            "intelligent  :  intellig\n",
            "computer  :  comput\n",
            "programs  :  program\n",
            "related  :  relat\n",
            "similar  :  similar\n",
            "task  :  task\n",
            "using  :  use\n",
            "computers  :  comput\n",
            "understand  :  understand\n",
            "human  :  human\n",
            "intelligence  :  intellig\n",
            "ai  :  ai\n",
            "confine  :  confin\n",
            "methods  :  method\n",
            "biologically  :  biolog\n",
            "observable  :  observ\n",
            "however  :  howev\n",
            "decades  :  decad\n",
            "definition  :  definit\n",
            "birth  :  birth\n",
            "artificial  :  artifici\n",
            "intelligence  :  intellig\n",
            "conversation  :  convers\n",
            "denoted  :  denot\n",
            "alan  :  alan\n",
            "turing  :  ture\n",
            "seminal  :  semin\n",
            "work  :  work\n",
            "computing  :  comput\n",
            "machinery  :  machineri\n",
            "intelligence  :  intellig\n",
            "pdf  :  pdf\n",
            "kb  :  kb\n",
            "link  :  link\n",
            "resides  :  resid\n",
            "outside  :  outsid\n",
            "ibm  :  ibm\n",
            "published  :  publish\n",
            "paper  :  paper\n",
            "turing  :  ture\n",
            "often  :  often\n",
            "referred  :  refer\n",
            "father  :  father\n",
            "computer  :  comput\n",
            "science  :  scienc\n",
            "asks  :  ask\n",
            "following  :  follow\n",
            "question  :  question\n",
            "machines  :  machin\n",
            "think  :  think\n",
            "offers  :  offer\n",
            "test  :  test\n",
            "famously  :  famous\n",
            "known  :  known\n",
            "turing  :  ture\n",
            "test  :  test\n",
            "human  :  human\n",
            "interrogator  :  interrog\n",
            "would  :  would\n",
            "try  :  tri\n",
            "distinguish  :  distinguish\n",
            "computer  :  comput\n",
            "human  :  human\n",
            "text  :  text\n",
            "response  :  respons\n",
            "test  :  test\n",
            "undergone  :  undergon\n",
            "much  :  much\n",
            "scrutiny  :  scrutini\n",
            "since  :  sinc\n",
            "publish  :  publish\n",
            "remains  :  remain\n",
            "important  :  import\n",
            "part  :  part\n",
            "history  :  histori\n",
            "ai  :  ai\n",
            "well  :  well\n",
            "ongoing  :  ongo\n",
            "concept  :  concept\n",
            "within  :  within\n",
            "philosophy  :  philosophi\n",
            "utilizes  :  util\n",
            "ideas  :  idea\n",
            "around  :  around\n",
            "linguistics  :  linguist\n",
            "stuart  :  stuart\n",
            "russell  :  russel\n",
            "peter  :  peter\n",
            "norvig  :  norvig\n",
            "proceeded  :  proceed\n",
            "publish  :  publish\n",
            "artificial  :  artifici\n",
            "intelligence  :  intellig\n",
            "modern  :  modern\n",
            "approach  :  approach\n",
            "link  :  link\n",
            "resides  :  resid\n",
            "outside  :  outsid\n",
            "ibm  :  ibm\n",
            "becoming  :  becom\n",
            "one  :  one\n",
            "leading  :  lead\n",
            "textbooks  :  textbook\n",
            "study  :  studi\n",
            "ai  :  ai\n",
            "delve  :  delv\n",
            "four  :  four\n",
            "potential  :  potenti\n",
            "goals  :  goal\n",
            "definitions  :  definit\n",
            "ai  :  ai\n",
            "differentiates  :  differenti\n",
            "computer  :  comput\n",
            "systems  :  system\n",
            "basis  :  basi\n",
            "rationality  :  ration\n",
            "thinking  :  think\n",
            "vs.  :  vs.\n",
            "acting  :  act\n",
            "\n",
            "lemmatize--------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "number ---> number\n",
            "definitions ---> definition\n",
            "artificial ---> artificial\n",
            "intelligence ---> intelligence\n",
            "ai ---> ai\n",
            "surfaced ---> surfaced\n",
            "last ---> last\n",
            "decades ---> decade\n",
            "john ---> john\n",
            "mccarthy ---> mccarthy\n",
            "offers ---> offer\n",
            "following ---> following\n",
            "definition ---> definition\n",
            "paper ---> paper\n",
            "pdf ---> pdf\n",
            "kb ---> kb\n",
            "link ---> link\n",
            "resides ---> resides\n",
            "outside ---> outside\n",
            "ibm ---> ibm\n",
            "science ---> science\n",
            "engineering ---> engineering\n",
            "making ---> making\n",
            "intelligent ---> intelligent\n",
            "machines ---> machine\n",
            "especially ---> especially\n",
            "intelligent ---> intelligent\n",
            "computer ---> computer\n",
            "programs ---> program\n",
            "related ---> related\n",
            "similar ---> similar\n",
            "task ---> task\n",
            "using ---> using\n",
            "computers ---> computer\n",
            "understand ---> understand\n",
            "human ---> human\n",
            "intelligence ---> intelligence\n",
            "ai ---> ai\n",
            "confine ---> confine\n",
            "methods ---> method\n",
            "biologically ---> biologically\n",
            "observable ---> observable\n",
            "however ---> however\n",
            "decades ---> decade\n",
            "definition ---> definition\n",
            "birth ---> birth\n",
            "artificial ---> artificial\n",
            "intelligence ---> intelligence\n",
            "conversation ---> conversation\n",
            "denoted ---> denoted\n",
            "alan ---> alan\n",
            "turing ---> turing\n",
            "seminal ---> seminal\n",
            "work ---> work\n",
            "computing ---> computing\n",
            "machinery ---> machinery\n",
            "intelligence ---> intelligence\n",
            "pdf ---> pdf\n",
            "kb ---> kb\n",
            "link ---> link\n",
            "resides ---> resides\n",
            "outside ---> outside\n",
            "ibm ---> ibm\n",
            "published ---> published\n",
            "paper ---> paper\n",
            "turing ---> turing\n",
            "often ---> often\n",
            "referred ---> referred\n",
            "father ---> father\n",
            "computer ---> computer\n",
            "science ---> science\n",
            "asks ---> asks\n",
            "following ---> following\n",
            "question ---> question\n",
            "machines ---> machine\n",
            "think ---> think\n",
            "offers ---> offer\n",
            "test ---> test\n",
            "famously ---> famously\n",
            "known ---> known\n",
            "turing ---> turing\n",
            "test ---> test\n",
            "human ---> human\n",
            "interrogator ---> interrogator\n",
            "would ---> would\n",
            "try ---> try\n",
            "distinguish ---> distinguish\n",
            "computer ---> computer\n",
            "human ---> human\n",
            "text ---> text\n",
            "response ---> response\n",
            "test ---> test\n",
            "undergone ---> undergone\n",
            "much ---> much\n",
            "scrutiny ---> scrutiny\n",
            "since ---> since\n",
            "publish ---> publish\n",
            "remains ---> remains\n",
            "important ---> important\n",
            "part ---> part\n",
            "history ---> history\n",
            "ai ---> ai\n",
            "well ---> well\n",
            "ongoing ---> ongoing\n",
            "concept ---> concept\n",
            "within ---> within\n",
            "philosophy ---> philosophy\n",
            "utilizes ---> utilizes\n",
            "ideas ---> idea\n",
            "around ---> around\n",
            "linguistics ---> linguistics\n",
            "stuart ---> stuart\n",
            "russell ---> russell\n",
            "peter ---> peter\n",
            "norvig ---> norvig\n",
            "proceeded ---> proceeded\n",
            "publish ---> publish\n",
            "artificial ---> artificial\n",
            "intelligence ---> intelligence\n",
            "modern ---> modern\n",
            "approach ---> approach\n",
            "link ---> link\n",
            "resides ---> resides\n",
            "outside ---> outside\n",
            "ibm ---> ibm\n",
            "becoming ---> becoming\n",
            "one ---> one\n",
            "leading ---> leading\n",
            "textbooks ---> textbook\n",
            "study ---> study\n",
            "ai ---> ai\n",
            "delve ---> delve\n",
            "four ---> four\n",
            "potential ---> potential\n",
            "goals ---> goal\n",
            "definitions ---> definition\n",
            "ai ---> ai\n",
            "differentiates ---> differentiates\n",
            "computer ---> computer\n",
            "systems ---> system\n",
            "basis ---> basis\n",
            "rationality ---> rationality\n",
            "thinking ---> thinking\n",
            "vs. ---> vs.\n",
            "acting ---> acting\n",
            "after token\n",
            "['Computer', 'science', 'is', 'the', 'study', 'of', 'computation', ',', 'automation', ',', 'and', 'information', '.', 'Computer', 'science', 'spans', 'theoretical', 'disciplines', '(', 'such', 'as', 'algorithms', ',', 'theory', 'of', 'computation', ',', 'and', 'information', 'theory', ')', 'to', 'practical', 'disciplines', '(', 'including', 'the', 'design', 'and', 'implementation', 'of', 'hardware', 'and', 'software', ')', 'Computer', 'science', 'is', 'generally', 'considered', 'an', 'area', 'of', 'academic', 'research', 'and', 'distinct', 'from', 'computer', 'programming', '.', 'Algorithms', 'and', 'data', 'structures', 'are', 'central', 'to', 'computer', 'science', '.', '[', '5', ']', 'The', 'theory', 'of', 'computation', 'concerns', 'abstract', 'models', 'of', 'computation', 'and', 'general', 'classes', 'of', 'problems', 'that', 'can', 'be', 'solved', 'using', 'them', '.', 'The', 'fields', 'of', 'cryptography', 'and', 'computer', 'security', 'involve', 'studying', 'the', 'means', 'for', 'secure', 'communication', 'and', 'for', 'preventing', 'security', 'vulnerabilities', '.', 'Computer', 'graphics', 'and', 'computational', 'geometry', 'address', 'the', 'generation', 'of', 'images', '.', 'Programming', 'language', 'theory', 'considers', 'approaches', 'to', 'the', 'description', 'of', 'computational', 'processes', ',', 'and', 'database', 'theory', 'concerns', 'the', 'management', 'of', 'repositories', 'of', 'data', '.', 'Human–computer', 'interaction', 'investigates', 'the', 'interfaces', 'through', 'which', 'humans', 'and', 'computers', 'interact', ',', 'and', 'software', 'engineering', 'focuses', 'on', 'the', 'design', 'and', 'principles', 'behind', 'developing', 'software', '.', 'Areas', 'such', 'as', 'operating', 'systems', ',', 'networks', 'and', 'embedded', 'systems', 'investigate', 'the', 'principles', 'and', 'design', 'behind', 'complex', 'systems', '.', 'Computer', 'architecture', 'describes', 'the', 'construction', 'of', 'computer', 'components', 'and', 'computer-operated', 'equipment', '.', 'Artificial', 'intelligence', 'and', 'machine', 'learning', 'aim', 'to', 'synthesize', 'goal-orientated', 'processes', 'such', 'as', 'problem-solving', ',', 'decision-making', ',', 'environmental', 'adaptation', ',', 'planning', 'and', 'learning', 'found', 'in', 'humans', 'and', 'animals', '.', 'Within', 'artificial', 'intelligence', ',', 'computer', 'vision', 'aims', 'to', 'understand', 'and', 'process', 'image', 'and', 'video', 'data', ',', 'while', 'natural-language', 'processing', 'aims', 'to', 'understand', 'and', 'process', 'textual', 'and', 'linguistic', 'data', '.', 'The', 'fundamental', 'concern', 'of', 'computer', 'science', 'is', 'determining', 'what', 'can', 'and', 'can', 'not', 'be', 'automated', '.', 'The', 'Turing', 'Award', 'is', 'generally', 'recognized', 'as', 'the', 'highest']\n",
            "after stopword removed----------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "['computer', 'science', 'study', 'computation', 'automation', 'information', 'computer', 'science', 'spans', 'theoretical', 'disciplines', 'algorithms', 'theory', 'computation', 'information', 'theory', 'practical', 'disciplines', 'including', 'design', 'implementation', 'hardware', 'software', 'computer', 'science', 'generally', 'considered', 'area', 'academic', 'research', 'distinct', 'computer', 'programming', 'algorithms', 'data', 'structures', 'central', 'computer', 'science', 'theory', 'computation', 'concerns', 'abstract', 'models', 'computation', 'general', 'classes', 'problems', 'solved', 'using', 'fields', 'cryptography', 'computer', 'security', 'involve', 'studying', 'means', 'secure', 'communication', 'preventing', 'security', 'vulnerabilities', 'computer', 'graphics', 'computational', 'geometry', 'address', 'generation', 'images', 'programming', 'language', 'theory', 'considers', 'approaches', 'description', 'computational', 'processes', 'database', 'theory', 'concerns', 'management', 'repositories', 'data', 'human–computer', 'interaction', 'investigates', 'interfaces', 'humans', 'computers', 'interact', 'software', 'engineering', 'focuses', 'design', 'principles', 'behind', 'developing', 'software', 'areas', 'operating', 'systems', 'networks', 'embedded', 'systems', 'investigate', 'principles', 'design', 'behind', 'complex', 'systems', 'computer', 'architecture', 'describes', 'construction', 'computer', 'components', 'computer-operated', 'equipment', 'artificial', 'intelligence', 'machine', 'learning', 'aim', 'synthesize', 'goal-orientated', 'processes', 'problem-solving', 'decision-making', 'environmental', 'adaptation', 'planning', 'learning', 'found', 'humans', 'animals', 'within', 'artificial', 'intelligence', 'computer', 'vision', 'aims', 'understand', 'process', 'image', 'video', 'data', 'natural-language', 'processing', 'aims', 'understand', 'process', 'textual', 'linguistic', 'data', 'fundamental', 'concern', 'computer', 'science', 'determining', 'automated', 'turing', 'award', 'generally', 'recognized', 'highest']\n",
            "stemming-----------------------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "computer  :  comput\n",
            "science  :  scienc\n",
            "study  :  studi\n",
            "computation  :  comput\n",
            "automation  :  autom\n",
            "information  :  inform\n",
            "computer  :  comput\n",
            "science  :  scienc\n",
            "spans  :  span\n",
            "theoretical  :  theoret\n",
            "disciplines  :  disciplin\n",
            "algorithms  :  algorithm\n",
            "theory  :  theori\n",
            "computation  :  comput\n",
            "information  :  inform\n",
            "theory  :  theori\n",
            "practical  :  practic\n",
            "disciplines  :  disciplin\n",
            "including  :  includ\n",
            "design  :  design\n",
            "implementation  :  implement\n",
            "hardware  :  hardwar\n",
            "software  :  softwar\n",
            "computer  :  comput\n",
            "science  :  scienc\n",
            "generally  :  gener\n",
            "considered  :  consid\n",
            "area  :  area\n",
            "academic  :  academ\n",
            "research  :  research\n",
            "distinct  :  distinct\n",
            "computer  :  comput\n",
            "programming  :  program\n",
            "algorithms  :  algorithm\n",
            "data  :  data\n",
            "structures  :  structur\n",
            "central  :  central\n",
            "computer  :  comput\n",
            "science  :  scienc\n",
            "theory  :  theori\n",
            "computation  :  comput\n",
            "concerns  :  concern\n",
            "abstract  :  abstract\n",
            "models  :  model\n",
            "computation  :  comput\n",
            "general  :  gener\n",
            "classes  :  class\n",
            "problems  :  problem\n",
            "solved  :  solv\n",
            "using  :  use\n",
            "fields  :  field\n",
            "cryptography  :  cryptographi\n",
            "computer  :  comput\n",
            "security  :  secur\n",
            "involve  :  involv\n",
            "studying  :  studi\n",
            "means  :  mean\n",
            "secure  :  secur\n",
            "communication  :  commun\n",
            "preventing  :  prevent\n",
            "security  :  secur\n",
            "vulnerabilities  :  vulner\n",
            "computer  :  comput\n",
            "graphics  :  graphic\n",
            "computational  :  comput\n",
            "geometry  :  geometri\n",
            "address  :  address\n",
            "generation  :  gener\n",
            "images  :  imag\n",
            "programming  :  program\n",
            "language  :  languag\n",
            "theory  :  theori\n",
            "considers  :  consid\n",
            "approaches  :  approach\n",
            "description  :  descript\n",
            "computational  :  comput\n",
            "processes  :  process\n",
            "database  :  databas\n",
            "theory  :  theori\n",
            "concerns  :  concern\n",
            "management  :  manag\n",
            "repositories  :  repositori\n",
            "data  :  data\n",
            "human–computer  :  human–comput\n",
            "interaction  :  interact\n",
            "investigates  :  investig\n",
            "interfaces  :  interfac\n",
            "humans  :  human\n",
            "computers  :  comput\n",
            "interact  :  interact\n",
            "software  :  softwar\n",
            "engineering  :  engin\n",
            "focuses  :  focus\n",
            "design  :  design\n",
            "principles  :  principl\n",
            "behind  :  behind\n",
            "developing  :  develop\n",
            "software  :  softwar\n",
            "areas  :  area\n",
            "operating  :  oper\n",
            "systems  :  system\n",
            "networks  :  network\n",
            "embedded  :  embed\n",
            "systems  :  system\n",
            "investigate  :  investig\n",
            "principles  :  principl\n",
            "design  :  design\n",
            "behind  :  behind\n",
            "complex  :  complex\n",
            "systems  :  system\n",
            "computer  :  comput\n",
            "architecture  :  architectur\n",
            "describes  :  describ\n",
            "construction  :  construct\n",
            "computer  :  comput\n",
            "components  :  compon\n",
            "computer-operated  :  computer-oper\n",
            "equipment  :  equip\n",
            "artificial  :  artifici\n",
            "intelligence  :  intellig\n",
            "machine  :  machin\n",
            "learning  :  learn\n",
            "aim  :  aim\n",
            "synthesize  :  synthes\n",
            "goal-orientated  :  goal-orient\n",
            "processes  :  process\n",
            "problem-solving  :  problem-solv\n",
            "decision-making  :  decision-mak\n",
            "environmental  :  environment\n",
            "adaptation  :  adapt\n",
            "planning  :  plan\n",
            "learning  :  learn\n",
            "found  :  found\n",
            "humans  :  human\n",
            "animals  :  anim\n",
            "within  :  within\n",
            "artificial  :  artifici\n",
            "intelligence  :  intellig\n",
            "computer  :  comput\n",
            "vision  :  vision\n",
            "aims  :  aim\n",
            "understand  :  understand\n",
            "process  :  process\n",
            "image  :  imag\n",
            "video  :  video\n",
            "data  :  data\n",
            "natural-language  :  natural-languag\n",
            "processing  :  process\n",
            "aims  :  aim\n",
            "understand  :  understand\n",
            "process  :  process\n",
            "textual  :  textual\n",
            "linguistic  :  linguist\n",
            "data  :  data\n",
            "fundamental  :  fundament\n",
            "concern  :  concern\n",
            "computer  :  comput\n",
            "science  :  scienc\n",
            "determining  :  determin\n",
            "automated  :  autom\n",
            "turing  :  ture\n",
            "award  :  award\n",
            "generally  :  gener\n",
            "recognized  :  recogn\n",
            "highest  :  highest\n",
            "\n",
            "lemmatize--------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "computer ---> computer\n",
            "science ---> science\n",
            "study ---> study\n",
            "computation ---> computation\n",
            "automation ---> automation\n",
            "information ---> information\n",
            "computer ---> computer\n",
            "science ---> science\n",
            "spans ---> span\n",
            "theoretical ---> theoretical\n",
            "disciplines ---> discipline\n",
            "algorithms ---> algorithm\n",
            "theory ---> theory\n",
            "computation ---> computation\n",
            "information ---> information\n",
            "theory ---> theory\n",
            "practical ---> practical\n",
            "disciplines ---> discipline\n",
            "including ---> including\n",
            "design ---> design\n",
            "implementation ---> implementation\n",
            "hardware ---> hardware\n",
            "software ---> software\n",
            "computer ---> computer\n",
            "science ---> science\n",
            "generally ---> generally\n",
            "considered ---> considered\n",
            "area ---> area\n",
            "academic ---> academic\n",
            "research ---> research\n",
            "distinct ---> distinct\n",
            "computer ---> computer\n",
            "programming ---> programming\n",
            "algorithms ---> algorithm\n",
            "data ---> data\n",
            "structures ---> structure\n",
            "central ---> central\n",
            "computer ---> computer\n",
            "science ---> science\n",
            "theory ---> theory\n",
            "computation ---> computation\n",
            "concerns ---> concern\n",
            "abstract ---> abstract\n",
            "models ---> model\n",
            "computation ---> computation\n",
            "general ---> general\n",
            "classes ---> class\n",
            "problems ---> problem\n",
            "solved ---> solved\n",
            "using ---> using\n",
            "fields ---> field\n",
            "cryptography ---> cryptography\n",
            "computer ---> computer\n",
            "security ---> security\n",
            "involve ---> involve\n",
            "studying ---> studying\n",
            "means ---> mean\n",
            "secure ---> secure\n",
            "communication ---> communication\n",
            "preventing ---> preventing\n",
            "security ---> security\n",
            "vulnerabilities ---> vulnerability\n",
            "computer ---> computer\n",
            "graphics ---> graphic\n",
            "computational ---> computational\n",
            "geometry ---> geometry\n",
            "address ---> address\n",
            "generation ---> generation\n",
            "images ---> image\n",
            "programming ---> programming\n",
            "language ---> language\n",
            "theory ---> theory\n",
            "considers ---> considers\n",
            "approaches ---> approach\n",
            "description ---> description\n",
            "computational ---> computational\n",
            "processes ---> process\n",
            "database ---> database\n",
            "theory ---> theory\n",
            "concerns ---> concern\n",
            "management ---> management\n",
            "repositories ---> repository\n",
            "data ---> data\n",
            "human–computer ---> human–computer\n",
            "interaction ---> interaction\n",
            "investigates ---> investigates\n",
            "interfaces ---> interface\n",
            "humans ---> human\n",
            "computers ---> computer\n",
            "interact ---> interact\n",
            "software ---> software\n",
            "engineering ---> engineering\n",
            "focuses ---> focus\n",
            "design ---> design\n",
            "principles ---> principle\n",
            "behind ---> behind\n",
            "developing ---> developing\n",
            "software ---> software\n",
            "areas ---> area\n",
            "operating ---> operating\n",
            "systems ---> system\n",
            "networks ---> network\n",
            "embedded ---> embedded\n",
            "systems ---> system\n",
            "investigate ---> investigate\n",
            "principles ---> principle\n",
            "design ---> design\n",
            "behind ---> behind\n",
            "complex ---> complex\n",
            "systems ---> system\n",
            "computer ---> computer\n",
            "architecture ---> architecture\n",
            "describes ---> describes\n",
            "construction ---> construction\n",
            "computer ---> computer\n",
            "components ---> component\n",
            "computer-operated ---> computer-operated\n",
            "equipment ---> equipment\n",
            "artificial ---> artificial\n",
            "intelligence ---> intelligence\n",
            "machine ---> machine\n",
            "learning ---> learning\n",
            "aim ---> aim\n",
            "synthesize ---> synthesize\n",
            "goal-orientated ---> goal-orientated\n",
            "processes ---> process\n",
            "problem-solving ---> problem-solving\n",
            "decision-making ---> decision-making\n",
            "environmental ---> environmental\n",
            "adaptation ---> adaptation\n",
            "planning ---> planning\n",
            "learning ---> learning\n",
            "found ---> found\n",
            "humans ---> human\n",
            "animals ---> animal\n",
            "within ---> within\n",
            "artificial ---> artificial\n",
            "intelligence ---> intelligence\n",
            "computer ---> computer\n",
            "vision ---> vision\n",
            "aims ---> aim\n",
            "understand ---> understand\n",
            "process ---> process\n",
            "image ---> image\n",
            "video ---> video\n",
            "data ---> data\n",
            "natural-language ---> natural-language\n",
            "processing ---> processing\n",
            "aims ---> aim\n",
            "understand ---> understand\n",
            "process ---> process\n",
            "textual ---> textual\n",
            "linguistic ---> linguistic\n",
            "data ---> data\n",
            "fundamental ---> fundamental\n",
            "concern ---> concern\n",
            "computer ---> computer\n",
            "science ---> science\n",
            "determining ---> determining\n",
            "automated ---> automated\n",
            "turing ---> turing\n",
            "award ---> award\n",
            "generally ---> generally\n",
            "recognized ---> recognized\n",
            "highest ---> highest\n",
            "after token\n",
            "['Computer', 'security', ',', 'cybersecurity', ',', 'or', 'information', 'technology', 'security', '(', 'IT', 'security', ')', 'is', 'the', 'protection', 'of', 'computer', 'systems', 'and', 'networks', 'from', 'information', 'disclosure', ',', 'theft', 'of', 'or', 'damage', 'to', 'their', 'hardware', ',', 'software', ',', 'or', 'electronic', 'data', ',', 'as', 'well', 'as', 'from', 'the', 'disruption', 'or', 'misdirection', 'of', 'the', 'services', 'they', 'provide', '.', '[', '1', ']', 'The', 'field', 'is', 'becoming', 'increasingly', 'significant', 'due', 'to', 'the', 'continuously', 'expanding', 'reliance', 'on', 'computer', 'systems', ',', 'the', 'Internet', '[', '2', ']', 'and', 'wireless', 'network', 'standards', 'such', 'as', 'Bluetooth', 'and', 'Wi-Fi', ',', 'and', 'due', 'to', 'the', 'growth', 'of', 'smart', 'devices', ',', 'including', 'smartphones', ',', 'televisions', ',', 'and', 'the', 'various', 'devices', 'that', 'constitute', 'the', 'Internet', 'of', 'things', '.', 'Cybersecurity', 'is', 'also', 'one', 'of', 'the', 'significant', 'challenges', 'in', 'the', 'contemporary', 'world', ',', 'due', 'to', 'its', 'complexity', ',', 'both', 'in', 'terms', 'of', 'political', 'usage', 'and', 'technology', '.', 'Its', 'primary', 'goal', 'is', 'to', 'ensure', 'the', 'system', 'dependability', ',', 'integrity', ',', 'and', 'data', 'privacy', '.', '[', '3', ']', '[', '4', ']']\n",
            "after stopword removed----------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "['computer', 'security', 'cybersecurity', 'information', 'technology', 'security', 'security', 'protection', 'computer', 'systems', 'networks', 'information', 'disclosure', 'theft', 'damage', 'hardware', 'software', 'electronic', 'data', 'well', 'disruption', 'misdirection', 'services', 'provide', 'field', 'becoming', 'increasingly', 'significant', 'due', 'continuously', 'expanding', 'reliance', 'computer', 'systems', 'internet', 'wireless', 'network', 'standards', 'bluetooth', 'wi-fi', 'due', 'growth', 'smart', 'devices', 'including', 'smartphones', 'televisions', 'various', 'devices', 'constitute', 'internet', 'things', 'cybersecurity', 'also', 'one', 'significant', 'challenges', 'contemporary', 'world', 'due', 'complexity', 'terms', 'political', 'usage', 'technology', 'primary', 'goal', 'ensure', 'system', 'dependability', 'integrity', 'data', 'privacy']\n",
            "stemming-----------------------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "computer  :  comput\n",
            "security  :  secur\n",
            "cybersecurity  :  cybersecur\n",
            "information  :  inform\n",
            "technology  :  technolog\n",
            "security  :  secur\n",
            "security  :  secur\n",
            "protection  :  protect\n",
            "computer  :  comput\n",
            "systems  :  system\n",
            "networks  :  network\n",
            "information  :  inform\n",
            "disclosure  :  disclosur\n",
            "theft  :  theft\n",
            "damage  :  damag\n",
            "hardware  :  hardwar\n",
            "software  :  softwar\n",
            "electronic  :  electron\n",
            "data  :  data\n",
            "well  :  well\n",
            "disruption  :  disrupt\n",
            "misdirection  :  misdirect\n",
            "services  :  servic\n",
            "provide  :  provid\n",
            "field  :  field\n",
            "becoming  :  becom\n",
            "increasingly  :  increasingli\n",
            "significant  :  signific\n",
            "due  :  due\n",
            "continuously  :  continu\n",
            "expanding  :  expand\n",
            "reliance  :  relianc\n",
            "computer  :  comput\n",
            "systems  :  system\n",
            "internet  :  internet\n",
            "wireless  :  wireless\n",
            "network  :  network\n",
            "standards  :  standard\n",
            "bluetooth  :  bluetooth\n",
            "wi-fi  :  wi-fi\n",
            "due  :  due\n",
            "growth  :  growth\n",
            "smart  :  smart\n",
            "devices  :  devic\n",
            "including  :  includ\n",
            "smartphones  :  smartphon\n",
            "televisions  :  televis\n",
            "various  :  variou\n",
            "devices  :  devic\n",
            "constitute  :  constitut\n",
            "internet  :  internet\n",
            "things  :  thing\n",
            "cybersecurity  :  cybersecur\n",
            "also  :  also\n",
            "one  :  one\n",
            "significant  :  signific\n",
            "challenges  :  challeng\n",
            "contemporary  :  contemporari\n",
            "world  :  world\n",
            "due  :  due\n",
            "complexity  :  complex\n",
            "terms  :  term\n",
            "political  :  polit\n",
            "usage  :  usag\n",
            "technology  :  technolog\n",
            "primary  :  primari\n",
            "goal  :  goal\n",
            "ensure  :  ensur\n",
            "system  :  system\n",
            "dependability  :  depend\n",
            "integrity  :  integr\n",
            "data  :  data\n",
            "privacy  :  privaci\n",
            "\n",
            "lemmatize--------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "computer ---> computer\n",
            "security ---> security\n",
            "cybersecurity ---> cybersecurity\n",
            "information ---> information\n",
            "technology ---> technology\n",
            "security ---> security\n",
            "security ---> security\n",
            "protection ---> protection\n",
            "computer ---> computer\n",
            "systems ---> system\n",
            "networks ---> network\n",
            "information ---> information\n",
            "disclosure ---> disclosure\n",
            "theft ---> theft\n",
            "damage ---> damage\n",
            "hardware ---> hardware\n",
            "software ---> software\n",
            "electronic ---> electronic\n",
            "data ---> data\n",
            "well ---> well\n",
            "disruption ---> disruption\n",
            "misdirection ---> misdirection\n",
            "services ---> service\n",
            "provide ---> provide\n",
            "field ---> field\n",
            "becoming ---> becoming\n",
            "increasingly ---> increasingly\n",
            "significant ---> significant\n",
            "due ---> due\n",
            "continuously ---> continuously\n",
            "expanding ---> expanding\n",
            "reliance ---> reliance\n",
            "computer ---> computer\n",
            "systems ---> system\n",
            "internet ---> internet\n",
            "wireless ---> wireless\n",
            "network ---> network\n",
            "standards ---> standard\n",
            "bluetooth ---> bluetooth\n",
            "wi-fi ---> wi-fi\n",
            "due ---> due\n",
            "growth ---> growth\n",
            "smart ---> smart\n",
            "devices ---> device\n",
            "including ---> including\n",
            "smartphones ---> smartphones\n",
            "televisions ---> television\n",
            "various ---> various\n",
            "devices ---> device\n",
            "constitute ---> constitute\n",
            "internet ---> internet\n",
            "things ---> thing\n",
            "cybersecurity ---> cybersecurity\n",
            "also ---> also\n",
            "one ---> one\n",
            "significant ---> significant\n",
            "challenges ---> challenge\n",
            "contemporary ---> contemporary\n",
            "world ---> world\n",
            "due ---> due\n",
            "complexity ---> complexity\n",
            "terms ---> term\n",
            "political ---> political\n",
            "usage ---> usage\n",
            "technology ---> technology\n",
            "primary ---> primary\n",
            "goal ---> goal\n",
            "ensure ---> ensure\n",
            "system ---> system\n",
            "dependability ---> dependability\n",
            "integrity ---> integrity\n",
            "data ---> data\n",
            "privacy ---> privacy\n",
            "after token\n",
            "['Cooking', ',', 'cookery', ',', 'or', 'culinary', 'arts', 'is', 'the', 'art', ',', 'science', ',', 'and', 'craft', 'of', 'using', 'heat', 'to', 'prepare', 'food', 'for', 'consumption', '.', 'Cooking', 'techniques', 'and', 'ingredients', 'vary', 'widely', ',', 'from', 'grilling', 'food', 'over', 'an', 'open', 'fire', 'to', 'using', 'electric', 'stoves', ',', 'to', 'baking', 'in', 'various', 'types', 'of', 'ovens', ',', 'reflecting', 'local', 'conditions', '.', 'Types', 'of', 'cooking', 'also', 'depend', 'on', 'the', 'skill', 'levels', 'and', 'training', 'of', 'the', 'cooks', '.', 'Cooking', 'is', 'done', 'both', 'by', 'people', 'in', 'their', 'own', 'dwellings', 'and', 'by', 'professional', 'cooks', 'and', 'chefs', 'in', 'restaurants', 'and', 'other', 'food', 'establishments', '.', 'Preparing', 'food', 'with', 'heat', 'or', 'fire', 'is', 'an', 'activity', 'unique', 'to', 'humans', '.', 'It', 'may', 'have', 'started', 'around', '2', 'million', 'years', 'ago', ',', 'though', 'archaeological', 'evidence', 'for', 'the', 'same', 'does', 'not', 'predate', 'more', 'than', '1', 'million', 'years', '.', 'The', 'expansion', 'of', 'agriculture', ',', 'commerce', ',', 'trade', ',', 'and', 'transportation', 'between', 'civilizations', 'in', 'different', 'regions', 'offered', 'cooks', 'many', 'new', 'ingredients', '.', 'New', 'inventions', 'and', 'technologies', ',', 'such', 'as', 'the', 'invention', 'of', 'pottery', 'for', 'holding', 'and', 'boiling', 'of', 'water', ',', 'expanded', 'cooking', 'techniques', '.', 'Some', 'modern', 'cooks', 'apply', 'advanced', 'scientific', 'techniques', 'to', 'food', 'preparation', 'to', 'further', 'enhance', 'the', 'flavor', 'of', 'the', 'dish', 'served', '.']\n",
            "after stopword removed----------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "['cooking', 'cookery', 'culinary', 'arts', 'art', 'science', 'craft', 'using', 'heat', 'prepare', 'food', 'consumption', 'cooking', 'techniques', 'ingredients', 'vary', 'widely', 'grilling', 'food', 'open', 'fire', 'using', 'electric', 'stoves', 'baking', 'various', 'types', 'ovens', 'reflecting', 'local', 'conditions', 'types', 'cooking', 'also', 'depend', 'skill', 'levels', 'training', 'cooks', 'cooking', 'done', 'people', 'dwellings', 'professional', 'cooks', 'chefs', 'restaurants', 'food', 'establishments', 'preparing', 'food', 'heat', 'fire', 'activity', 'unique', 'humans', 'may', 'started', 'around', 'million', 'years', 'ago', 'though', 'archaeological', 'evidence', 'predate', 'million', 'years', 'expansion', 'agriculture', 'commerce', 'trade', 'transportation', 'civilizations', 'different', 'regions', 'offered', 'cooks', 'many', 'new', 'ingredients', 'new', 'inventions', 'technologies', 'invention', 'pottery', 'holding', 'boiling', 'water', 'expanded', 'cooking', 'techniques', 'modern', 'cooks', 'apply', 'advanced', 'scientific', 'techniques', 'food', 'preparation', 'enhance', 'flavor', 'dish', 'served']\n",
            "stemming-----------------------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "cooking  :  cook\n",
            "cookery  :  cookeri\n",
            "culinary  :  culinari\n",
            "arts  :  art\n",
            "art  :  art\n",
            "science  :  scienc\n",
            "craft  :  craft\n",
            "using  :  use\n",
            "heat  :  heat\n",
            "prepare  :  prepar\n",
            "food  :  food\n",
            "consumption  :  consumpt\n",
            "cooking  :  cook\n",
            "techniques  :  techniqu\n",
            "ingredients  :  ingredi\n",
            "vary  :  vari\n",
            "widely  :  wide\n",
            "grilling  :  grill\n",
            "food  :  food\n",
            "open  :  open\n",
            "fire  :  fire\n",
            "using  :  use\n",
            "electric  :  electr\n",
            "stoves  :  stove\n",
            "baking  :  bake\n",
            "various  :  variou\n",
            "types  :  type\n",
            "ovens  :  oven\n",
            "reflecting  :  reflect\n",
            "local  :  local\n",
            "conditions  :  condit\n",
            "types  :  type\n",
            "cooking  :  cook\n",
            "also  :  also\n",
            "depend  :  depend\n",
            "skill  :  skill\n",
            "levels  :  level\n",
            "training  :  train\n",
            "cooks  :  cook\n",
            "cooking  :  cook\n",
            "done  :  done\n",
            "people  :  peopl\n",
            "dwellings  :  dwell\n",
            "professional  :  profession\n",
            "cooks  :  cook\n",
            "chefs  :  chef\n",
            "restaurants  :  restaur\n",
            "food  :  food\n",
            "establishments  :  establish\n",
            "preparing  :  prepar\n",
            "food  :  food\n",
            "heat  :  heat\n",
            "fire  :  fire\n",
            "activity  :  activ\n",
            "unique  :  uniqu\n",
            "humans  :  human\n",
            "may  :  may\n",
            "started  :  start\n",
            "around  :  around\n",
            "million  :  million\n",
            "years  :  year\n",
            "ago  :  ago\n",
            "though  :  though\n",
            "archaeological  :  archaeolog\n",
            "evidence  :  evid\n",
            "predate  :  predat\n",
            "million  :  million\n",
            "years  :  year\n",
            "expansion  :  expans\n",
            "agriculture  :  agricultur\n",
            "commerce  :  commerc\n",
            "trade  :  trade\n",
            "transportation  :  transport\n",
            "civilizations  :  civil\n",
            "different  :  differ\n",
            "regions  :  region\n",
            "offered  :  offer\n",
            "cooks  :  cook\n",
            "many  :  mani\n",
            "new  :  new\n",
            "ingredients  :  ingredi\n",
            "new  :  new\n",
            "inventions  :  invent\n",
            "technologies  :  technolog\n",
            "invention  :  invent\n",
            "pottery  :  potteri\n",
            "holding  :  hold\n",
            "boiling  :  boil\n",
            "water  :  water\n",
            "expanded  :  expand\n",
            "cooking  :  cook\n",
            "techniques  :  techniqu\n",
            "modern  :  modern\n",
            "cooks  :  cook\n",
            "apply  :  appli\n",
            "advanced  :  advanc\n",
            "scientific  :  scientif\n",
            "techniques  :  techniqu\n",
            "food  :  food\n",
            "preparation  :  prepar\n",
            "enhance  :  enhanc\n",
            "flavor  :  flavor\n",
            "dish  :  dish\n",
            "served  :  serv\n",
            "\n",
            "lemmatize--------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "cooking ---> cooking\n",
            "cookery ---> cookery\n",
            "culinary ---> culinary\n",
            "arts ---> art\n",
            "art ---> art\n",
            "science ---> science\n",
            "craft ---> craft\n",
            "using ---> using\n",
            "heat ---> heat\n",
            "prepare ---> prepare\n",
            "food ---> food\n",
            "consumption ---> consumption\n",
            "cooking ---> cooking\n",
            "techniques ---> technique\n",
            "ingredients ---> ingredient\n",
            "vary ---> vary\n",
            "widely ---> widely\n",
            "grilling ---> grilling\n",
            "food ---> food\n",
            "open ---> open\n",
            "fire ---> fire\n",
            "using ---> using\n",
            "electric ---> electric\n",
            "stoves ---> stove\n",
            "baking ---> baking\n",
            "various ---> various\n",
            "types ---> type\n",
            "ovens ---> oven\n",
            "reflecting ---> reflecting\n",
            "local ---> local\n",
            "conditions ---> condition\n",
            "types ---> type\n",
            "cooking ---> cooking\n",
            "also ---> also\n",
            "depend ---> depend\n",
            "skill ---> skill\n",
            "levels ---> level\n",
            "training ---> training\n",
            "cooks ---> cook\n",
            "cooking ---> cooking\n",
            "done ---> done\n",
            "people ---> people\n",
            "dwellings ---> dwelling\n",
            "professional ---> professional\n",
            "cooks ---> cook\n",
            "chefs ---> chef\n",
            "restaurants ---> restaurant\n",
            "food ---> food\n",
            "establishments ---> establishment\n",
            "preparing ---> preparing\n",
            "food ---> food\n",
            "heat ---> heat\n",
            "fire ---> fire\n",
            "activity ---> activity\n",
            "unique ---> unique\n",
            "humans ---> human\n",
            "may ---> may\n",
            "started ---> started\n",
            "around ---> around\n",
            "million ---> million\n",
            "years ---> year\n",
            "ago ---> ago\n",
            "though ---> though\n",
            "archaeological ---> archaeological\n",
            "evidence ---> evidence\n",
            "predate ---> predate\n",
            "million ---> million\n",
            "years ---> year\n",
            "expansion ---> expansion\n",
            "agriculture ---> agriculture\n",
            "commerce ---> commerce\n",
            "trade ---> trade\n",
            "transportation ---> transportation\n",
            "civilizations ---> civilization\n",
            "different ---> different\n",
            "regions ---> region\n",
            "offered ---> offered\n",
            "cooks ---> cook\n",
            "many ---> many\n",
            "new ---> new\n",
            "ingredients ---> ingredient\n",
            "new ---> new\n",
            "inventions ---> invention\n",
            "technologies ---> technology\n",
            "invention ---> invention\n",
            "pottery ---> pottery\n",
            "holding ---> holding\n",
            "boiling ---> boiling\n",
            "water ---> water\n",
            "expanded ---> expanded\n",
            "cooking ---> cooking\n",
            "techniques ---> technique\n",
            "modern ---> modern\n",
            "cooks ---> cook\n",
            "apply ---> apply\n",
            "advanced ---> advanced\n",
            "scientific ---> scientific\n",
            "techniques ---> technique\n",
            "food ---> food\n",
            "preparation ---> preparation\n",
            "enhance ---> enhance\n",
            "flavor ---> flavor\n",
            "dish ---> dish\n",
            "served ---> served\n",
            "after token\n",
            "['Data', 'science', 'is', 'an', 'interdisciplinary', 'field', 'that', 'uses', 'scientific', 'methods', ',', 'processes', ',', 'algorithms', 'and', 'systems', 'to', 'extract', 'knowledge', 'and', 'insights', 'from', 'noisy', ',', 'structured', 'and', 'unstructured', 'data', ',', '[', '1', ']', '[', '2', ']', 'and', 'apply', 'knowledge', 'and', 'actionable', 'insights', 'from', 'data', 'across', 'a', 'broad', 'range', 'of', 'application', 'domains', '.', 'Data', 'science', 'is', 'related', 'to', 'data', 'mining', ',', 'machine', 'learning', 'and', 'big', 'data', '.', 'Data', 'science', 'is', 'a', 'concept', 'to', 'unify', 'statistics', ',', 'data', 'analysis', ',', 'informatics', ',', 'and', 'their', 'related', 'methods', 'in', 'order', 'to', 'understand', 'and', 'analyze', 'actual', 'phenomena', 'with', 'data', '.', '[', '3', ']', 'It', 'uses', 'techniques', 'and', 'theories', 'drawn', 'from', 'many', 'fields', 'within', 'the', 'context', 'of', 'mathematics', ',', 'statistics', ',', 'computer', 'science', ',', 'information', 'science', ',', 'and', 'domain', 'knowledge', '.', '[', '4', ']', 'However', ',', 'data', 'science', 'is', 'different', 'from', 'computer', 'science', 'and', 'information', 'science', '.', 'Turing', 'Award', 'winner', 'Jim', 'Gray', 'imagined', 'data', 'science', 'as', 'a', 'fourth', 'paradigm', 'of', 'science', '(', 'empirical', ',', 'theoretical', ',', 'computational', ',', 'and', 'now', 'data-driven', ')', 'and', 'asserted', 'that', 'everything', 'about', 'science', 'is', 'changing', 'because', 'of', 'the', 'impact', 'of', 'information', 'technology', 'and', 'the', 'data', 'deluge', '.', '[', '5', ']', '[', '6', ']', 'A', 'data', 'scientist', 'is', 'someone', 'who', 'creates', 'programming', 'code', ',', 'and', 'combines', 'it', 'with', 'statistical', 'knowledge', 'to', 'create', 'insights']\n",
            "after stopword removed----------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "['data', 'science', 'interdisciplinary', 'field', 'uses', 'scientific', 'methods', 'processes', 'algorithms', 'systems', 'extract', 'knowledge', 'insights', 'noisy', 'structured', 'unstructured', 'data', 'apply', 'knowledge', 'actionable', 'insights', 'data', 'across', 'broad', 'range', 'application', 'domains', 'data', 'science', 'related', 'data', 'mining', 'machine', 'learning', 'big', 'data', 'data', 'science', 'concept', 'unify', 'statistics', 'data', 'analysis', 'informatics', 'related', 'methods', 'order', 'understand', 'analyze', 'actual', 'phenomena', 'data', 'uses', 'techniques', 'theories', 'drawn', 'many', 'fields', 'within', 'context', 'mathematics', 'statistics', 'computer', 'science', 'information', 'science', 'domain', 'knowledge', 'however', 'data', 'science', 'different', 'computer', 'science', 'information', 'science', 'turing', 'award', 'winner', 'jim', 'gray', 'imagined', 'data', 'science', 'fourth', 'paradigm', 'science', 'empirical', 'theoretical', 'computational', 'data-driven', 'asserted', 'everything', 'science', 'changing', 'impact', 'information', 'technology', 'data', 'deluge', 'data', 'scientist', 'someone', 'creates', 'programming', 'code', 'combines', 'statistical', 'knowledge', 'create', 'insights']\n",
            "stemming-----------------------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "data  :  data\n",
            "science  :  scienc\n",
            "interdisciplinary  :  interdisciplinari\n",
            "field  :  field\n",
            "uses  :  use\n",
            "scientific  :  scientif\n",
            "methods  :  method\n",
            "processes  :  process\n",
            "algorithms  :  algorithm\n",
            "systems  :  system\n",
            "extract  :  extract\n",
            "knowledge  :  knowledg\n",
            "insights  :  insight\n",
            "noisy  :  noisi\n",
            "structured  :  structur\n",
            "unstructured  :  unstructur\n",
            "data  :  data\n",
            "apply  :  appli\n",
            "knowledge  :  knowledg\n",
            "actionable  :  action\n",
            "insights  :  insight\n",
            "data  :  data\n",
            "across  :  across\n",
            "broad  :  broad\n",
            "range  :  rang\n",
            "application  :  applic\n",
            "domains  :  domain\n",
            "data  :  data\n",
            "science  :  scienc\n",
            "related  :  relat\n",
            "data  :  data\n",
            "mining  :  mine\n",
            "machine  :  machin\n",
            "learning  :  learn\n",
            "big  :  big\n",
            "data  :  data\n",
            "data  :  data\n",
            "science  :  scienc\n",
            "concept  :  concept\n",
            "unify  :  unifi\n",
            "statistics  :  statist\n",
            "data  :  data\n",
            "analysis  :  analysi\n",
            "informatics  :  informat\n",
            "related  :  relat\n",
            "methods  :  method\n",
            "order  :  order\n",
            "understand  :  understand\n",
            "analyze  :  analyz\n",
            "actual  :  actual\n",
            "phenomena  :  phenomena\n",
            "data  :  data\n",
            "uses  :  use\n",
            "techniques  :  techniqu\n",
            "theories  :  theori\n",
            "drawn  :  drawn\n",
            "many  :  mani\n",
            "fields  :  field\n",
            "within  :  within\n",
            "context  :  context\n",
            "mathematics  :  mathemat\n",
            "statistics  :  statist\n",
            "computer  :  comput\n",
            "science  :  scienc\n",
            "information  :  inform\n",
            "science  :  scienc\n",
            "domain  :  domain\n",
            "knowledge  :  knowledg\n",
            "however  :  howev\n",
            "data  :  data\n",
            "science  :  scienc\n",
            "different  :  differ\n",
            "computer  :  comput\n",
            "science  :  scienc\n",
            "information  :  inform\n",
            "science  :  scienc\n",
            "turing  :  ture\n",
            "award  :  award\n",
            "winner  :  winner\n",
            "jim  :  jim\n",
            "gray  :  gray\n",
            "imagined  :  imagin\n",
            "data  :  data\n",
            "science  :  scienc\n",
            "fourth  :  fourth\n",
            "paradigm  :  paradigm\n",
            "science  :  scienc\n",
            "empirical  :  empir\n",
            "theoretical  :  theoret\n",
            "computational  :  comput\n",
            "data-driven  :  data-driven\n",
            "asserted  :  assert\n",
            "everything  :  everyth\n",
            "science  :  scienc\n",
            "changing  :  chang\n",
            "impact  :  impact\n",
            "information  :  inform\n",
            "technology  :  technolog\n",
            "data  :  data\n",
            "deluge  :  delug\n",
            "data  :  data\n",
            "scientist  :  scientist\n",
            "someone  :  someon\n",
            "creates  :  creat\n",
            "programming  :  program\n",
            "code  :  code\n",
            "combines  :  combin\n",
            "statistical  :  statist\n",
            "knowledge  :  knowledg\n",
            "create  :  creat\n",
            "insights  :  insight\n",
            "\n",
            "lemmatize--------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "data ---> data\n",
            "science ---> science\n",
            "interdisciplinary ---> interdisciplinary\n",
            "field ---> field\n",
            "uses ---> us\n",
            "scientific ---> scientific\n",
            "methods ---> method\n",
            "processes ---> process\n",
            "algorithms ---> algorithm\n",
            "systems ---> system\n",
            "extract ---> extract\n",
            "knowledge ---> knowledge\n",
            "insights ---> insight\n",
            "noisy ---> noisy\n",
            "structured ---> structured\n",
            "unstructured ---> unstructured\n",
            "data ---> data\n",
            "apply ---> apply\n",
            "knowledge ---> knowledge\n",
            "actionable ---> actionable\n",
            "insights ---> insight\n",
            "data ---> data\n",
            "across ---> across\n",
            "broad ---> broad\n",
            "range ---> range\n",
            "application ---> application\n",
            "domains ---> domain\n",
            "data ---> data\n",
            "science ---> science\n",
            "related ---> related\n",
            "data ---> data\n",
            "mining ---> mining\n",
            "machine ---> machine\n",
            "learning ---> learning\n",
            "big ---> big\n",
            "data ---> data\n",
            "data ---> data\n",
            "science ---> science\n",
            "concept ---> concept\n",
            "unify ---> unify\n",
            "statistics ---> statistic\n",
            "data ---> data\n",
            "analysis ---> analysis\n",
            "informatics ---> informatics\n",
            "related ---> related\n",
            "methods ---> method\n",
            "order ---> order\n",
            "understand ---> understand\n",
            "analyze ---> analyze\n",
            "actual ---> actual\n",
            "phenomena ---> phenomenon\n",
            "data ---> data\n",
            "uses ---> us\n",
            "techniques ---> technique\n",
            "theories ---> theory\n",
            "drawn ---> drawn\n",
            "many ---> many\n",
            "fields ---> field\n",
            "within ---> within\n",
            "context ---> context\n",
            "mathematics ---> mathematics\n",
            "statistics ---> statistic\n",
            "computer ---> computer\n",
            "science ---> science\n",
            "information ---> information\n",
            "science ---> science\n",
            "domain ---> domain\n",
            "knowledge ---> knowledge\n",
            "however ---> however\n",
            "data ---> data\n",
            "science ---> science\n",
            "different ---> different\n",
            "computer ---> computer\n",
            "science ---> science\n",
            "information ---> information\n",
            "science ---> science\n",
            "turing ---> turing\n",
            "award ---> award\n",
            "winner ---> winner\n",
            "jim ---> jim\n",
            "gray ---> gray\n",
            "imagined ---> imagined\n",
            "data ---> data\n",
            "science ---> science\n",
            "fourth ---> fourth\n",
            "paradigm ---> paradigm\n",
            "science ---> science\n",
            "empirical ---> empirical\n",
            "theoretical ---> theoretical\n",
            "computational ---> computational\n",
            "data-driven ---> data-driven\n",
            "asserted ---> asserted\n",
            "everything ---> everything\n",
            "science ---> science\n",
            "changing ---> changing\n",
            "impact ---> impact\n",
            "information ---> information\n",
            "technology ---> technology\n",
            "data ---> data\n",
            "deluge ---> deluge\n",
            "data ---> data\n",
            "scientist ---> scientist\n",
            "someone ---> someone\n",
            "creates ---> creates\n",
            "programming ---> programming\n",
            "code ---> code\n",
            "combines ---> combine\n",
            "statistical ---> statistical\n",
            "knowledge ---> knowledge\n",
            "create ---> create\n",
            "insights ---> insight\n",
            "after token\n",
            "['Machine', 'Learning', 'A', 'computer', 'learns', 'when', 'its', 'software', 'is', 'able', 'to', 'successfully', 'predict', 'and', 'react', 'to', 'unfolding', 'scenarios', 'based', 'on', 'previous', 'outcomes', '.', 'Machine', 'learning', 'refers', 'to', 'the', 'process', 'by', 'which', 'computers', 'develop', 'pattern', 'recognition', ',', 'or', 'the', 'ability', 'to', 'continuously', 'learn', 'from', 'and', 'make', 'predictions', 'based', 'on', 'data', ',', 'and', 'can', 'make', 'adjustments', 'without', 'being', 'specifically', 'programmed', 'to', 'do', 'so', '.', 'A', 'form', 'of', 'artificial', 'intelligence', ',', 'machine', 'learning', 'effectively', 'automates', 'the', 'process', 'of', 'analytical', 'model-building', 'and', 'allows', 'machines', 'to', 'adapt', 'to', 'new', 'scenarios', 'independently', '.', 'The', 'four', 'steps', 'for', 'building', 'a', 'machine', 'learning', 'model', 'are', ':', '1', 'Select', 'and', 'prepare', 'a', 'training', 'data', 'set', 'necessary', 'to', 'solving', 'the', 'problem', '.', 'This', 'data', 'can', 'be', 'labeled', 'or', 'unlabeled', '.', '2', 'Choose', 'an', 'algorithm', 'to', 'run', 'on', 'the', 'training', 'data', '.', 'If', 'the', 'data', 'is', 'labeled', ',', 'the', 'algorithm', 'could', 'be', 'regression', ',', 'decision', 'trees', ',', 'or', 'instance-based', '.', 'If', 'the', 'data', 'is', 'unlabeled', ',', 'the', 'algorithm', 'could', 'be', 'a', 'clustering', 'algorithm', ',', 'an', 'association', 'algorithm', ',', 'or', 'a', 'neural', 'network', '.', '3', 'Train', 'the', 'algorithm', 'to', 'create', 'the', 'model', '.', '4', 'Use', 'and', 'improve', 'the', 'model', '.', 'There', 'are', 'three', 'methods', 'of', 'machine', 'learning', ':', 'Supervised', 'learning', 'works', 'with', 'labeled', 'data', 'and', 'requires', 'less', 'training', '.', 'Unsupervised', 'learning', 'is', 'used', 'to', 'classify', 'unlabeled', 'data', 'by', 'identifying', 'patterns', 'and', 'relationships', '.', 'Semi-supervised', 'learning', 'uses', 'a', 'small', 'labeled', 'data', 'set', 'to', 'guide', 'classification', 'of', 'a', 'larger', 'unlabeled', 'data', 'set', '.']\n",
            "after stopword removed----------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "['machine', 'learning', 'computer', 'learns', 'software', 'able', 'successfully', 'predict', 'react', 'unfolding', 'scenarios', 'based', 'previous', 'outcomes', 'machine', 'learning', 'refers', 'process', 'computers', 'develop', 'pattern', 'recognition', 'ability', 'continuously', 'learn', 'make', 'predictions', 'based', 'data', 'make', 'adjustments', 'without', 'specifically', 'programmed', 'form', 'artificial', 'intelligence', 'machine', 'learning', 'effectively', 'automates', 'process', 'analytical', 'model-building', 'allows', 'machines', 'adapt', 'new', 'scenarios', 'independently', 'four', 'steps', 'building', 'machine', 'learning', 'model', 'select', 'prepare', 'training', 'data', 'set', 'necessary', 'solving', 'problem', 'data', 'labeled', 'unlabeled', 'choose', 'algorithm', 'run', 'training', 'data', 'data', 'labeled', 'algorithm', 'could', 'regression', 'decision', 'trees', 'instance-based', 'data', 'unlabeled', 'algorithm', 'could', 'clustering', 'algorithm', 'association', 'algorithm', 'neural', 'network', 'train', 'algorithm', 'create', 'model', 'use', 'improve', 'model', 'three', 'methods', 'machine', 'learning', 'supervised', 'learning', 'works', 'labeled', 'data', 'requires', 'less', 'training', 'unsupervised', 'learning', 'used', 'classify', 'unlabeled', 'data', 'identifying', 'patterns', 'relationships', 'semi-supervised', 'learning', 'uses', 'small', 'labeled', 'data', 'set', 'guide', 'classification', 'larger', 'unlabeled', 'data', 'set']\n",
            "stemming-----------------------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "machine  :  machin\n",
            "learning  :  learn\n",
            "computer  :  comput\n",
            "learns  :  learn\n",
            "software  :  softwar\n",
            "able  :  abl\n",
            "successfully  :  success\n",
            "predict  :  predict\n",
            "react  :  react\n",
            "unfolding  :  unfold\n",
            "scenarios  :  scenario\n",
            "based  :  base\n",
            "previous  :  previou\n",
            "outcomes  :  outcom\n",
            "machine  :  machin\n",
            "learning  :  learn\n",
            "refers  :  refer\n",
            "process  :  process\n",
            "computers  :  comput\n",
            "develop  :  develop\n",
            "pattern  :  pattern\n",
            "recognition  :  recognit\n",
            "ability  :  abil\n",
            "continuously  :  continu\n",
            "learn  :  learn\n",
            "make  :  make\n",
            "predictions  :  predict\n",
            "based  :  base\n",
            "data  :  data\n",
            "make  :  make\n",
            "adjustments  :  adjust\n",
            "without  :  without\n",
            "specifically  :  specif\n",
            "programmed  :  program\n",
            "form  :  form\n",
            "artificial  :  artifici\n",
            "intelligence  :  intellig\n",
            "machine  :  machin\n",
            "learning  :  learn\n",
            "effectively  :  effect\n",
            "automates  :  autom\n",
            "process  :  process\n",
            "analytical  :  analyt\n",
            "model-building  :  model-build\n",
            "allows  :  allow\n",
            "machines  :  machin\n",
            "adapt  :  adapt\n",
            "new  :  new\n",
            "scenarios  :  scenario\n",
            "independently  :  independ\n",
            "four  :  four\n",
            "steps  :  step\n",
            "building  :  build\n",
            "machine  :  machin\n",
            "learning  :  learn\n",
            "model  :  model\n",
            "select  :  select\n",
            "prepare  :  prepar\n",
            "training  :  train\n",
            "data  :  data\n",
            "set  :  set\n",
            "necessary  :  necessari\n",
            "solving  :  solv\n",
            "problem  :  problem\n",
            "data  :  data\n",
            "labeled  :  label\n",
            "unlabeled  :  unlabel\n",
            "choose  :  choos\n",
            "algorithm  :  algorithm\n",
            "run  :  run\n",
            "training  :  train\n",
            "data  :  data\n",
            "data  :  data\n",
            "labeled  :  label\n",
            "algorithm  :  algorithm\n",
            "could  :  could\n",
            "regression  :  regress\n",
            "decision  :  decis\n",
            "trees  :  tree\n",
            "instance-based  :  instance-bas\n",
            "data  :  data\n",
            "unlabeled  :  unlabel\n",
            "algorithm  :  algorithm\n",
            "could  :  could\n",
            "clustering  :  cluster\n",
            "algorithm  :  algorithm\n",
            "association  :  associ\n",
            "algorithm  :  algorithm\n",
            "neural  :  neural\n",
            "network  :  network\n",
            "train  :  train\n",
            "algorithm  :  algorithm\n",
            "create  :  creat\n",
            "model  :  model\n",
            "use  :  use\n",
            "improve  :  improv\n",
            "model  :  model\n",
            "three  :  three\n",
            "methods  :  method\n",
            "machine  :  machin\n",
            "learning  :  learn\n",
            "supervised  :  supervis\n",
            "learning  :  learn\n",
            "works  :  work\n",
            "labeled  :  label\n",
            "data  :  data\n",
            "requires  :  requir\n",
            "less  :  less\n",
            "training  :  train\n",
            "unsupervised  :  unsupervis\n",
            "learning  :  learn\n",
            "used  :  use\n",
            "classify  :  classifi\n",
            "unlabeled  :  unlabel\n",
            "data  :  data\n",
            "identifying  :  identifi\n",
            "patterns  :  pattern\n",
            "relationships  :  relationship\n",
            "semi-supervised  :  semi-supervis\n",
            "learning  :  learn\n",
            "uses  :  use\n",
            "small  :  small\n",
            "labeled  :  label\n",
            "data  :  data\n",
            "set  :  set\n",
            "guide  :  guid\n",
            "classification  :  classif\n",
            "larger  :  larger\n",
            "unlabeled  :  unlabel\n",
            "data  :  data\n",
            "set  :  set\n",
            "\n",
            "lemmatize--------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "machine ---> machine\n",
            "learning ---> learning\n",
            "computer ---> computer\n",
            "learns ---> learns\n",
            "software ---> software\n",
            "able ---> able\n",
            "successfully ---> successfully\n",
            "predict ---> predict\n",
            "react ---> react\n",
            "unfolding ---> unfolding\n",
            "scenarios ---> scenario\n",
            "based ---> based\n",
            "previous ---> previous\n",
            "outcomes ---> outcome\n",
            "machine ---> machine\n",
            "learning ---> learning\n",
            "refers ---> refers\n",
            "process ---> process\n",
            "computers ---> computer\n",
            "develop ---> develop\n",
            "pattern ---> pattern\n",
            "recognition ---> recognition\n",
            "ability ---> ability\n",
            "continuously ---> continuously\n",
            "learn ---> learn\n",
            "make ---> make\n",
            "predictions ---> prediction\n",
            "based ---> based\n",
            "data ---> data\n",
            "make ---> make\n",
            "adjustments ---> adjustment\n",
            "without ---> without\n",
            "specifically ---> specifically\n",
            "programmed ---> programmed\n",
            "form ---> form\n",
            "artificial ---> artificial\n",
            "intelligence ---> intelligence\n",
            "machine ---> machine\n",
            "learning ---> learning\n",
            "effectively ---> effectively\n",
            "automates ---> automates\n",
            "process ---> process\n",
            "analytical ---> analytical\n",
            "model-building ---> model-building\n",
            "allows ---> allows\n",
            "machines ---> machine\n",
            "adapt ---> adapt\n",
            "new ---> new\n",
            "scenarios ---> scenario\n",
            "independently ---> independently\n",
            "four ---> four\n",
            "steps ---> step\n",
            "building ---> building\n",
            "machine ---> machine\n",
            "learning ---> learning\n",
            "model ---> model\n",
            "select ---> select\n",
            "prepare ---> prepare\n",
            "training ---> training\n",
            "data ---> data\n",
            "set ---> set\n",
            "necessary ---> necessary\n",
            "solving ---> solving\n",
            "problem ---> problem\n",
            "data ---> data\n",
            "labeled ---> labeled\n",
            "unlabeled ---> unlabeled\n",
            "choose ---> choose\n",
            "algorithm ---> algorithm\n",
            "run ---> run\n",
            "training ---> training\n",
            "data ---> data\n",
            "data ---> data\n",
            "labeled ---> labeled\n",
            "algorithm ---> algorithm\n",
            "could ---> could\n",
            "regression ---> regression\n",
            "decision ---> decision\n",
            "trees ---> tree\n",
            "instance-based ---> instance-based\n",
            "data ---> data\n",
            "unlabeled ---> unlabeled\n",
            "algorithm ---> algorithm\n",
            "could ---> could\n",
            "clustering ---> clustering\n",
            "algorithm ---> algorithm\n",
            "association ---> association\n",
            "algorithm ---> algorithm\n",
            "neural ---> neural\n",
            "network ---> network\n",
            "train ---> train\n",
            "algorithm ---> algorithm\n",
            "create ---> create\n",
            "model ---> model\n",
            "use ---> use\n",
            "improve ---> improve\n",
            "model ---> model\n",
            "three ---> three\n",
            "methods ---> method\n",
            "machine ---> machine\n",
            "learning ---> learning\n",
            "supervised ---> supervised\n",
            "learning ---> learning\n",
            "works ---> work\n",
            "labeled ---> labeled\n",
            "data ---> data\n",
            "requires ---> requires\n",
            "less ---> le\n",
            "training ---> training\n",
            "unsupervised ---> unsupervised\n",
            "learning ---> learning\n",
            "used ---> used\n",
            "classify ---> classify\n",
            "unlabeled ---> unlabeled\n",
            "data ---> data\n",
            "identifying ---> identifying\n",
            "patterns ---> pattern\n",
            "relationships ---> relationship\n",
            "semi-supervised ---> semi-supervised\n",
            "learning ---> learning\n",
            "uses ---> us\n",
            "small ---> small\n",
            "labeled ---> labeled\n",
            "data ---> data\n",
            "set ---> set\n",
            "guide ---> guide\n",
            "classification ---> classification\n",
            "larger ---> larger\n",
            "unlabeled ---> unlabeled\n",
            "data ---> data\n",
            "set ---> set\n",
            "after token\n",
            "['Natural', 'language', 'processing', '(', 'NLP', ')', 'refers', 'to', 'the', 'branch', 'of', 'computer', 'science—and', 'more', 'specifically', ',', 'the', 'branch', 'of', 'artificial', 'intelligence', 'or', 'AI—concerned', 'with', 'giving', 'computers', 'the', 'ability', 'to', 'understand', 'text', 'and', 'spoken', 'words', 'in', 'much', 'the', 'same', 'way', 'human', 'beings', 'can', '.', 'NLP', 'combines', 'computational', 'linguistics—rule-based', 'modeling', 'of', 'human', 'language—with', 'statistical', ',', 'machine', 'learning', ',', 'and', 'deep', 'learning', 'models', '.', 'Together', ',', 'these', 'technologies', 'enable', 'computers', 'to', 'process', 'human', 'language', 'in', 'the', 'form', 'of', 'text', 'or', 'voice', 'data', 'and', 'to', 'understand', 'its', 'full', 'meaning', ',', 'complete', 'with', 'the', 'speaker', 'or', 'writer', 'is', 'intent', 'and', 'sentiment', '.', 'NLP', 'drives', 'computer', 'programs', 'that', 'translate', 'text', 'from', 'one', 'language', 'to', 'another', ',', 'respond', 'to', 'spoken', 'commands', ',', 'and', 'summarize', 'large', 'volumes', 'of', 'text', 'rapidly—even', 'in', 'real', 'time', '.', 'There', 'is', 'a', 'good', 'chance', 'you', 'have', 'interacted', 'with', 'NLP', 'in', 'the', 'form', 'of', 'voice-operated', 'GPS', 'systems', ',', 'digital', 'assistants', ',', 'speech-to-text', 'dictation', 'software', ',', 'customer', 'service', 'chatbots', ',', 'and', 'other', 'consumer', 'conveniences', '.', 'But', 'NLP', 'also', 'plays', 'a', 'growing', 'role', 'in', 'enterprise', 'solutions', 'that', 'help', 'streamline', 'business', 'operations', ',', 'increase', 'employee', 'productivity', ',', 'and', 'simplify', 'mission-critical', 'business', 'processes', '.', 'NLP', 'tasks']\n",
            "after stopword removed----------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "['natural', 'language', 'processing', 'nlp', 'refers', 'branch', 'computer', 'science—and', 'specifically', 'branch', 'artificial', 'intelligence', 'ai—concerned', 'giving', 'computers', 'ability', 'understand', 'text', 'spoken', 'words', 'much', 'way', 'human', 'beings', 'nlp', 'combines', 'computational', 'linguistics—rule-based', 'modeling', 'human', 'language—with', 'statistical', 'machine', 'learning', 'deep', 'learning', 'models', 'together', 'technologies', 'enable', 'computers', 'process', 'human', 'language', 'form', 'text', 'voice', 'data', 'understand', 'full', 'meaning', 'complete', 'speaker', 'writer', 'intent', 'sentiment', 'nlp', 'drives', 'computer', 'programs', 'translate', 'text', 'one', 'language', 'another', 'respond', 'spoken', 'commands', 'summarize', 'large', 'volumes', 'text', 'rapidly—even', 'real', 'time', 'good', 'chance', 'interacted', 'nlp', 'form', 'voice-operated', 'gps', 'systems', 'digital', 'assistants', 'speech-to-text', 'dictation', 'software', 'customer', 'service', 'chatbots', 'consumer', 'conveniences', 'nlp', 'also', 'plays', 'growing', 'role', 'enterprise', 'solutions', 'help', 'streamline', 'business', 'operations', 'increase', 'employee', 'productivity', 'simplify', 'mission-critical', 'business', 'processes', 'nlp', 'tasks']\n",
            "stemming-----------------------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "natural  :  natur\n",
            "language  :  languag\n",
            "processing  :  process\n",
            "nlp  :  nlp\n",
            "refers  :  refer\n",
            "branch  :  branch\n",
            "computer  :  comput\n",
            "science—and  :  science—and\n",
            "specifically  :  specif\n",
            "branch  :  branch\n",
            "artificial  :  artifici\n",
            "intelligence  :  intellig\n",
            "ai—concerned  :  ai—concern\n",
            "giving  :  give\n",
            "computers  :  comput\n",
            "ability  :  abil\n",
            "understand  :  understand\n",
            "text  :  text\n",
            "spoken  :  spoken\n",
            "words  :  word\n",
            "much  :  much\n",
            "way  :  way\n",
            "human  :  human\n",
            "beings  :  be\n",
            "nlp  :  nlp\n",
            "combines  :  combin\n",
            "computational  :  comput\n",
            "linguistics—rule-based  :  linguistics—rule-bas\n",
            "modeling  :  model\n",
            "human  :  human\n",
            "language—with  :  language—with\n",
            "statistical  :  statist\n",
            "machine  :  machin\n",
            "learning  :  learn\n",
            "deep  :  deep\n",
            "learning  :  learn\n",
            "models  :  model\n",
            "together  :  togeth\n",
            "technologies  :  technolog\n",
            "enable  :  enabl\n",
            "computers  :  comput\n",
            "process  :  process\n",
            "human  :  human\n",
            "language  :  languag\n",
            "form  :  form\n",
            "text  :  text\n",
            "voice  :  voic\n",
            "data  :  data\n",
            "understand  :  understand\n",
            "full  :  full\n",
            "meaning  :  mean\n",
            "complete  :  complet\n",
            "speaker  :  speaker\n",
            "writer  :  writer\n",
            "intent  :  intent\n",
            "sentiment  :  sentiment\n",
            "nlp  :  nlp\n",
            "drives  :  drive\n",
            "computer  :  comput\n",
            "programs  :  program\n",
            "translate  :  translat\n",
            "text  :  text\n",
            "one  :  one\n",
            "language  :  languag\n",
            "another  :  anoth\n",
            "respond  :  respond\n",
            "spoken  :  spoken\n",
            "commands  :  command\n",
            "summarize  :  summar\n",
            "large  :  larg\n",
            "volumes  :  volum\n",
            "text  :  text\n",
            "rapidly—even  :  rapidly—even\n",
            "real  :  real\n",
            "time  :  time\n",
            "good  :  good\n",
            "chance  :  chanc\n",
            "interacted  :  interact\n",
            "nlp  :  nlp\n",
            "form  :  form\n",
            "voice-operated  :  voice-oper\n",
            "gps  :  gp\n",
            "systems  :  system\n",
            "digital  :  digit\n",
            "assistants  :  assist\n",
            "speech-to-text  :  speech-to-text\n",
            "dictation  :  dictat\n",
            "software  :  softwar\n",
            "customer  :  custom\n",
            "service  :  servic\n",
            "chatbots  :  chatbot\n",
            "consumer  :  consum\n",
            "conveniences  :  conveni\n",
            "nlp  :  nlp\n",
            "also  :  also\n",
            "plays  :  play\n",
            "growing  :  grow\n",
            "role  :  role\n",
            "enterprise  :  enterpris\n",
            "solutions  :  solut\n",
            "help  :  help\n",
            "streamline  :  streamlin\n",
            "business  :  busi\n",
            "operations  :  oper\n",
            "increase  :  increas\n",
            "employee  :  employe\n",
            "productivity  :  product\n",
            "simplify  :  simplifi\n",
            "mission-critical  :  mission-crit\n",
            "business  :  busi\n",
            "processes  :  process\n",
            "nlp  :  nlp\n",
            "tasks  :  task\n",
            "\n",
            "lemmatize--------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "natural ---> natural\n",
            "language ---> language\n",
            "processing ---> processing\n",
            "nlp ---> nlp\n",
            "refers ---> refers\n",
            "branch ---> branch\n",
            "computer ---> computer\n",
            "science—and ---> science—and\n",
            "specifically ---> specifically\n",
            "branch ---> branch\n",
            "artificial ---> artificial\n",
            "intelligence ---> intelligence\n",
            "ai—concerned ---> ai—concerned\n",
            "giving ---> giving\n",
            "computers ---> computer\n",
            "ability ---> ability\n",
            "understand ---> understand\n",
            "text ---> text\n",
            "spoken ---> spoken\n",
            "words ---> word\n",
            "much ---> much\n",
            "way ---> way\n",
            "human ---> human\n",
            "beings ---> being\n",
            "nlp ---> nlp\n",
            "combines ---> combine\n",
            "computational ---> computational\n",
            "linguistics—rule-based ---> linguistics—rule-based\n",
            "modeling ---> modeling\n",
            "human ---> human\n",
            "language—with ---> language—with\n",
            "statistical ---> statistical\n",
            "machine ---> machine\n",
            "learning ---> learning\n",
            "deep ---> deep\n",
            "learning ---> learning\n",
            "models ---> model\n",
            "together ---> together\n",
            "technologies ---> technology\n",
            "enable ---> enable\n",
            "computers ---> computer\n",
            "process ---> process\n",
            "human ---> human\n",
            "language ---> language\n",
            "form ---> form\n",
            "text ---> text\n",
            "voice ---> voice\n",
            "data ---> data\n",
            "understand ---> understand\n",
            "full ---> full\n",
            "meaning ---> meaning\n",
            "complete ---> complete\n",
            "speaker ---> speaker\n",
            "writer ---> writer\n",
            "intent ---> intent\n",
            "sentiment ---> sentiment\n",
            "nlp ---> nlp\n",
            "drives ---> drive\n",
            "computer ---> computer\n",
            "programs ---> program\n",
            "translate ---> translate\n",
            "text ---> text\n",
            "one ---> one\n",
            "language ---> language\n",
            "another ---> another\n",
            "respond ---> respond\n",
            "spoken ---> spoken\n",
            "commands ---> command\n",
            "summarize ---> summarize\n",
            "large ---> large\n",
            "volumes ---> volume\n",
            "text ---> text\n",
            "rapidly—even ---> rapidly—even\n",
            "real ---> real\n",
            "time ---> time\n",
            "good ---> good\n",
            "chance ---> chance\n",
            "interacted ---> interacted\n",
            "nlp ---> nlp\n",
            "form ---> form\n",
            "voice-operated ---> voice-operated\n",
            "gps ---> gps\n",
            "systems ---> system\n",
            "digital ---> digital\n",
            "assistants ---> assistant\n",
            "speech-to-text ---> speech-to-text\n",
            "dictation ---> dictation\n",
            "software ---> software\n",
            "customer ---> customer\n",
            "service ---> service\n",
            "chatbots ---> chatbots\n",
            "consumer ---> consumer\n",
            "conveniences ---> convenience\n",
            "nlp ---> nlp\n",
            "also ---> also\n",
            "plays ---> play\n",
            "growing ---> growing\n",
            "role ---> role\n",
            "enterprise ---> enterprise\n",
            "solutions ---> solution\n",
            "help ---> help\n",
            "streamline ---> streamline\n",
            "business ---> business\n",
            "operations ---> operation\n",
            "increase ---> increase\n",
            "employee ---> employee\n",
            "productivity ---> productivity\n",
            "simplify ---> simplify\n",
            "mission-critical ---> mission-critical\n",
            "business ---> business\n",
            "processes ---> process\n",
            "nlp ---> nlp\n",
            "tasks ---> task\n",
            "after token\n",
            "['Deep', 'Learning', 'Deep', 'learning', 'is', 'a', 'subset', 'of', 'machine', 'learning', 'that', 'has', 'demonstrated', 'significantly', 'superior', 'performance', 'to', 'some', 'traditional', 'machine', 'learning', 'approaches', '.', 'Deep', 'learning', 'utilizes', 'a', 'combination', 'of', 'multi', 'layer', 'artificial', 'neural', 'networks', 'and', 'data', 'and', 'compute-intensive', 'training', ',', 'inspired', 'by', 'our', 'latest', 'understanding', 'of', 'human', 'brain', 'behavior', '.', 'This', 'approach', 'has', 'become', 'so', 'effective', 'it', 'is', 'even', 'begun', 'to', 'surpass', 'human', 'abilities', 'in', 'many', 'areas', ',', 'such', 'as', 'image', 'and', 'speech', 'recognition', 'and', 'natural', 'language', 'processing', '.', 'Deep', 'learning', 'models', 'process', 'large', 'amounts', 'of', 'data', 'and', 'are', 'typically', 'unsupervised', 'or', 'semi-supervised']\n",
            "after stopword removed----------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "['deep', 'learning', 'deep', 'learning', 'subset', 'machine', 'learning', 'demonstrated', 'significantly', 'superior', 'performance', 'traditional', 'machine', 'learning', 'approaches', 'deep', 'learning', 'utilizes', 'combination', 'multi', 'layer', 'artificial', 'neural', 'networks', 'data', 'compute-intensive', 'training', 'inspired', 'latest', 'understanding', 'human', 'brain', 'behavior', 'approach', 'become', 'effective', 'even', 'begun', 'surpass', 'human', 'abilities', 'many', 'areas', 'image', 'speech', 'recognition', 'natural', 'language', 'processing', 'deep', 'learning', 'models', 'process', 'large', 'amounts', 'data', 'typically', 'unsupervised', 'semi-supervised']\n",
            "stemming-----------------------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "deep  :  deep\n",
            "learning  :  learn\n",
            "deep  :  deep\n",
            "learning  :  learn\n",
            "subset  :  subset\n",
            "machine  :  machin\n",
            "learning  :  learn\n",
            "demonstrated  :  demonstr\n",
            "significantly  :  significantli\n",
            "superior  :  superior\n",
            "performance  :  perform\n",
            "traditional  :  tradit\n",
            "machine  :  machin\n",
            "learning  :  learn\n",
            "approaches  :  approach\n",
            "deep  :  deep\n",
            "learning  :  learn\n",
            "utilizes  :  util\n",
            "combination  :  combin\n",
            "multi  :  multi\n",
            "layer  :  layer\n",
            "artificial  :  artifici\n",
            "neural  :  neural\n",
            "networks  :  network\n",
            "data  :  data\n",
            "compute-intensive  :  compute-intens\n",
            "training  :  train\n",
            "inspired  :  inspir\n",
            "latest  :  latest\n",
            "understanding  :  understand\n",
            "human  :  human\n",
            "brain  :  brain\n",
            "behavior  :  behavior\n",
            "approach  :  approach\n",
            "become  :  becom\n",
            "effective  :  effect\n",
            "even  :  even\n",
            "begun  :  begun\n",
            "surpass  :  surpass\n",
            "human  :  human\n",
            "abilities  :  abil\n",
            "many  :  mani\n",
            "areas  :  area\n",
            "image  :  imag\n",
            "speech  :  speech\n",
            "recognition  :  recognit\n",
            "natural  :  natur\n",
            "language  :  languag\n",
            "processing  :  process\n",
            "deep  :  deep\n",
            "learning  :  learn\n",
            "models  :  model\n",
            "process  :  process\n",
            "large  :  larg\n",
            "amounts  :  amount\n",
            "data  :  data\n",
            "typically  :  typic\n",
            "unsupervised  :  unsupervis\n",
            "semi-supervised  :  semi-supervis\n",
            "\n",
            "lemmatize--------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "deep ---> deep\n",
            "learning ---> learning\n",
            "deep ---> deep\n",
            "learning ---> learning\n",
            "subset ---> subset\n",
            "machine ---> machine\n",
            "learning ---> learning\n",
            "demonstrated ---> demonstrated\n",
            "significantly ---> significantly\n",
            "superior ---> superior\n",
            "performance ---> performance\n",
            "traditional ---> traditional\n",
            "machine ---> machine\n",
            "learning ---> learning\n",
            "approaches ---> approach\n",
            "deep ---> deep\n",
            "learning ---> learning\n",
            "utilizes ---> utilizes\n",
            "combination ---> combination\n",
            "multi ---> multi\n",
            "layer ---> layer\n",
            "artificial ---> artificial\n",
            "neural ---> neural\n",
            "networks ---> network\n",
            "data ---> data\n",
            "compute-intensive ---> compute-intensive\n",
            "training ---> training\n",
            "inspired ---> inspired\n",
            "latest ---> latest\n",
            "understanding ---> understanding\n",
            "human ---> human\n",
            "brain ---> brain\n",
            "behavior ---> behavior\n",
            "approach ---> approach\n",
            "become ---> become\n",
            "effective ---> effective\n",
            "even ---> even\n",
            "begun ---> begun\n",
            "surpass ---> surpass\n",
            "human ---> human\n",
            "abilities ---> ability\n",
            "many ---> many\n",
            "areas ---> area\n",
            "image ---> image\n",
            "speech ---> speech\n",
            "recognition ---> recognition\n",
            "natural ---> natural\n",
            "language ---> language\n",
            "processing ---> processing\n",
            "deep ---> deep\n",
            "learning ---> learning\n",
            "models ---> model\n",
            "process ---> process\n",
            "large ---> large\n",
            "amounts ---> amount\n",
            "data ---> data\n",
            "typically ---> typically\n",
            "unsupervised ---> unsupervised\n",
            "semi-supervised ---> semi-supervised\n",
            "after token\n",
            "['The', 'process', 'of', 'digging', 'through', 'data', 'to', 'discover', 'hidden', 'connections', 'and', 'predict', 'future', 'trends', 'has', 'a', 'long', 'history', '.', 'Sometimes', 'referred', 'to', 'as', 'knowledge', 'discovery', 'in', 'databases', ',', 'the', 'term', 'data', 'mining', 'was', 'not', 'coined', 'until', 'the', '1990', '.', 'But', 'its', 'foundation', 'comprises', 'three', 'intertwined', 'scientific', 'disciplines', ':', 'statistics', '(', 'the', 'numeric', 'study', 'of', 'data', 'relationships', ')', ',', 'artificial', 'intelligence', '(', 'human-like', 'intelligence', 'displayed', 'by', 'software', 'and/or', 'machines', ')', 'and', 'machine', 'learning', '(', 'algorithms', 'that', 'can', 'learn', 'from', 'data', 'to', 'make', 'predictions', ')', '.', 'What', 'was', 'old', 'is', 'new', 'again', ',', 'as', 'data', 'mining', 'technology', 'keeps', 'evolving', 'to', 'keep', 'pace', 'with', 'the', 'limitless', 'potential', 'of', 'big', 'data', 'and', 'affordable', 'computing', 'power', '.', 'Over', 'the', 'last', 'decade', ',', 'advances', 'in', 'processing', 'power', 'and', 'speed', 'have', 'enabled', 'us', 'to', 'move', 'beyond', 'manual', ',', 'tedious', 'and', 'time-consuming', 'practices', 'to', 'quick', ',', 'easy', 'and', 'automated', 'data', 'analysis', '.', 'The', 'more', 'complex', 'the', 'data', 'sets', 'collected', ',', 'the', 'more', 'potential', 'there', 'is', 'to', 'uncover', 'relevant', 'insights', '.', 'Retailers', ',', 'banks', ',', 'manufacturers', ',', 'telecommunications', 'providers', 'and', 'insurers', ',', 'among', 'others', ',', 'are', 'using', 'data', 'mining', 'to', 'discover', 'relationships', 'among', 'everything', 'from', 'price', 'optimization', ',', 'promotions', 'and', 'demographics', 'to', 'how', 'the', 'economy', ',', 'risk', ',', 'competition', 'and', 'social', 'media', 'are', 'affecting', 'their', 'business', 'models', ',', 'revenues', ',', 'operations', 'and', 'customer', 'relationships', '.']\n",
            "after stopword removed----------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "['process', 'digging', 'data', 'discover', 'hidden', 'connections', 'predict', 'future', 'trends', 'long', 'history', 'sometimes', 'referred', 'knowledge', 'discovery', 'databases', 'term', 'data', 'mining', 'coined', 'foundation', 'comprises', 'three', 'intertwined', 'scientific', 'disciplines', 'statistics', 'numeric', 'study', 'data', 'relationships', 'artificial', 'intelligence', 'human-like', 'intelligence', 'displayed', 'software', 'and/or', 'machines', 'machine', 'learning', 'algorithms', 'learn', 'data', 'make', 'predictions', 'old', 'new', 'data', 'mining', 'technology', 'keeps', 'evolving', 'keep', 'pace', 'limitless', 'potential', 'big', 'data', 'affordable', 'computing', 'power', 'last', 'decade', 'advances', 'processing', 'power', 'speed', 'enabled', 'us', 'move', 'beyond', 'manual', 'tedious', 'time-consuming', 'practices', 'quick', 'easy', 'automated', 'data', 'analysis', 'complex', 'data', 'sets', 'collected', 'potential', 'uncover', 'relevant', 'insights', 'retailers', 'banks', 'manufacturers', 'telecommunications', 'providers', 'insurers', 'among', 'others', 'using', 'data', 'mining', 'discover', 'relationships', 'among', 'everything', 'price', 'optimization', 'promotions', 'demographics', 'economy', 'risk', 'competition', 'social', 'media', 'affecting', 'business', 'models', 'revenues', 'operations', 'customer', 'relationships']\n",
            "stemming-----------------------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "process  :  process\n",
            "digging  :  dig\n",
            "data  :  data\n",
            "discover  :  discov\n",
            "hidden  :  hidden\n",
            "connections  :  connect\n",
            "predict  :  predict\n",
            "future  :  futur\n",
            "trends  :  trend\n",
            "long  :  long\n",
            "history  :  histori\n",
            "sometimes  :  sometim\n",
            "referred  :  refer\n",
            "knowledge  :  knowledg\n",
            "discovery  :  discoveri\n",
            "databases  :  databas\n",
            "term  :  term\n",
            "data  :  data\n",
            "mining  :  mine\n",
            "coined  :  coin\n",
            "foundation  :  foundat\n",
            "comprises  :  compris\n",
            "three  :  three\n",
            "intertwined  :  intertwin\n",
            "scientific  :  scientif\n",
            "disciplines  :  disciplin\n",
            "statistics  :  statist\n",
            "numeric  :  numer\n",
            "study  :  studi\n",
            "data  :  data\n",
            "relationships  :  relationship\n",
            "artificial  :  artifici\n",
            "intelligence  :  intellig\n",
            "human-like  :  human-lik\n",
            "intelligence  :  intellig\n",
            "displayed  :  display\n",
            "software  :  softwar\n",
            "and/or  :  and/or\n",
            "machines  :  machin\n",
            "machine  :  machin\n",
            "learning  :  learn\n",
            "algorithms  :  algorithm\n",
            "learn  :  learn\n",
            "data  :  data\n",
            "make  :  make\n",
            "predictions  :  predict\n",
            "old  :  old\n",
            "new  :  new\n",
            "data  :  data\n",
            "mining  :  mine\n",
            "technology  :  technolog\n",
            "keeps  :  keep\n",
            "evolving  :  evolv\n",
            "keep  :  keep\n",
            "pace  :  pace\n",
            "limitless  :  limitless\n",
            "potential  :  potenti\n",
            "big  :  big\n",
            "data  :  data\n",
            "affordable  :  afford\n",
            "computing  :  comput\n",
            "power  :  power\n",
            "last  :  last\n",
            "decade  :  decad\n",
            "advances  :  advanc\n",
            "processing  :  process\n",
            "power  :  power\n",
            "speed  :  speed\n",
            "enabled  :  enabl\n",
            "us  :  us\n",
            "move  :  move\n",
            "beyond  :  beyond\n",
            "manual  :  manual\n",
            "tedious  :  tediou\n",
            "time-consuming  :  time-consum\n",
            "practices  :  practic\n",
            "quick  :  quick\n",
            "easy  :  easi\n",
            "automated  :  autom\n",
            "data  :  data\n",
            "analysis  :  analysi\n",
            "complex  :  complex\n",
            "data  :  data\n",
            "sets  :  set\n",
            "collected  :  collect\n",
            "potential  :  potenti\n",
            "uncover  :  uncov\n",
            "relevant  :  relev\n",
            "insights  :  insight\n",
            "retailers  :  retail\n",
            "banks  :  bank\n",
            "manufacturers  :  manufactur\n",
            "telecommunications  :  telecommun\n",
            "providers  :  provid\n",
            "insurers  :  insur\n",
            "among  :  among\n",
            "others  :  other\n",
            "using  :  use\n",
            "data  :  data\n",
            "mining  :  mine\n",
            "discover  :  discov\n",
            "relationships  :  relationship\n",
            "among  :  among\n",
            "everything  :  everyth\n",
            "price  :  price\n",
            "optimization  :  optim\n",
            "promotions  :  promot\n",
            "demographics  :  demograph\n",
            "economy  :  economi\n",
            "risk  :  risk\n",
            "competition  :  competit\n",
            "social  :  social\n",
            "media  :  media\n",
            "affecting  :  affect\n",
            "business  :  busi\n",
            "models  :  model\n",
            "revenues  :  revenu\n",
            "operations  :  oper\n",
            "customer  :  custom\n",
            "relationships  :  relationship\n",
            "\n",
            "lemmatize--------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "process ---> process\n",
            "digging ---> digging\n",
            "data ---> data\n",
            "discover ---> discover\n",
            "hidden ---> hidden\n",
            "connections ---> connection\n",
            "predict ---> predict\n",
            "future ---> future\n",
            "trends ---> trend\n",
            "long ---> long\n",
            "history ---> history\n",
            "sometimes ---> sometimes\n",
            "referred ---> referred\n",
            "knowledge ---> knowledge\n",
            "discovery ---> discovery\n",
            "databases ---> database\n",
            "term ---> term\n",
            "data ---> data\n",
            "mining ---> mining\n",
            "coined ---> coined\n",
            "foundation ---> foundation\n",
            "comprises ---> comprises\n",
            "three ---> three\n",
            "intertwined ---> intertwined\n",
            "scientific ---> scientific\n",
            "disciplines ---> discipline\n",
            "statistics ---> statistic\n",
            "numeric ---> numeric\n",
            "study ---> study\n",
            "data ---> data\n",
            "relationships ---> relationship\n",
            "artificial ---> artificial\n",
            "intelligence ---> intelligence\n",
            "human-like ---> human-like\n",
            "intelligence ---> intelligence\n",
            "displayed ---> displayed\n",
            "software ---> software\n",
            "and/or ---> and/or\n",
            "machines ---> machine\n",
            "machine ---> machine\n",
            "learning ---> learning\n",
            "algorithms ---> algorithm\n",
            "learn ---> learn\n",
            "data ---> data\n",
            "make ---> make\n",
            "predictions ---> prediction\n",
            "old ---> old\n",
            "new ---> new\n",
            "data ---> data\n",
            "mining ---> mining\n",
            "technology ---> technology\n",
            "keeps ---> keep\n",
            "evolving ---> evolving\n",
            "keep ---> keep\n",
            "pace ---> pace\n",
            "limitless ---> limitless\n",
            "potential ---> potential\n",
            "big ---> big\n",
            "data ---> data\n",
            "affordable ---> affordable\n",
            "computing ---> computing\n",
            "power ---> power\n",
            "last ---> last\n",
            "decade ---> decade\n",
            "advances ---> advance\n",
            "processing ---> processing\n",
            "power ---> power\n",
            "speed ---> speed\n",
            "enabled ---> enabled\n",
            "us ---> u\n",
            "move ---> move\n",
            "beyond ---> beyond\n",
            "manual ---> manual\n",
            "tedious ---> tedious\n",
            "time-consuming ---> time-consuming\n",
            "practices ---> practice\n",
            "quick ---> quick\n",
            "easy ---> easy\n",
            "automated ---> automated\n",
            "data ---> data\n",
            "analysis ---> analysis\n",
            "complex ---> complex\n",
            "data ---> data\n",
            "sets ---> set\n",
            "collected ---> collected\n",
            "potential ---> potential\n",
            "uncover ---> uncover\n",
            "relevant ---> relevant\n",
            "insights ---> insight\n",
            "retailers ---> retailer\n",
            "banks ---> bank\n",
            "manufacturers ---> manufacturer\n",
            "telecommunications ---> telecommunication\n",
            "providers ---> provider\n",
            "insurers ---> insurer\n",
            "among ---> among\n",
            "others ---> others\n",
            "using ---> using\n",
            "data ---> data\n",
            "mining ---> mining\n",
            "discover ---> discover\n",
            "relationships ---> relationship\n",
            "among ---> among\n",
            "everything ---> everything\n",
            "price ---> price\n",
            "optimization ---> optimization\n",
            "promotions ---> promotion\n",
            "demographics ---> demographic\n",
            "economy ---> economy\n",
            "risk ---> risk\n",
            "competition ---> competition\n",
            "social ---> social\n",
            "media ---> medium\n",
            "affecting ---> affecting\n",
            "business ---> business\n",
            "models ---> model\n",
            "revenues ---> revenue\n",
            "operations ---> operation\n",
            "customer ---> customer\n",
            "relationships ---> relationship\n",
            "after token\n",
            "['The', 'brain', 'is', 'one', 'of', 'the', 'largest', 'and', 'most', 'complex', 'organs', 'in', 'the', 'human', 'body', '.', 'It', 'is', 'made', 'up', 'of', 'more', 'than', '100', 'billion', 'nerves', 'that', 'communicate', 'in', 'trillions', 'of', 'connections', 'called', 'synapses', '.', 'The', 'brain', 'is', 'made', 'up', 'of', 'many', 'specialized', 'areas', 'that', 'work', 'together', ':', 'The', 'cortex', 'is', 'the', 'outermost', 'layer', 'of', 'brain', 'cells', '.', 'Thinking', 'and', 'voluntary', 'movements', 'begin', 'in', 'the', 'cortex', '.', 'The', 'brain', 'stem', 'is', 'between', 'the', 'spinal', 'cord', 'and', 'the', 'rest', 'of', 'the', 'brain', '.', 'Basic', 'functions', 'like', 'breathing', 'and', 'sleep', 'are', 'controlled', 'here', '.', 'The', 'basal', 'ganglia', 'are', 'a', 'cluster', 'of', 'structures', 'in', 'the', 'center', 'of', 'the', 'brain', '.', 'The', 'basal', 'ganglia', 'coordinate', 'messages', 'between', 'multiple', 'other', 'brain', 'areas', '.', 'The', 'cerebellum', 'is', 'at', 'the', 'base', 'and', 'the', 'back', 'of', 'the', 'brain', '.', 'The', 'cerebellum', 'is', 'responsible', 'for', 'coordination', 'and', 'balance', '.', 'The', 'brain', 'is', 'also', 'divided', 'into', 'several', 'lobes', ':', 'The', 'frontal', 'lobes', 'are', 'responsible', 'for', 'problem', 'solving', 'and', 'judgment', 'and', 'motor', 'function', '.', 'The', 'parietal', 'lobes', 'manage', 'sensation', ',', 'handwriting', ',', 'and', 'body', 'position', '.', 'The', 'temporal', 'lobes', 'are', 'involved', 'with', 'memory', 'and', 'hearing', '.', 'The', 'occipital', 'lobes', 'contain', 'the', 'brain', 'is', 'visual', 'processing', 'system', '.']\n",
            "after stopword removed----------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "['brain', 'one', 'largest', 'complex', 'organs', 'human', 'body', 'made', 'billion', 'nerves', 'communicate', 'trillions', 'connections', 'called', 'synapses', 'brain', 'made', 'many', 'specialized', 'areas', 'work', 'together', 'cortex', 'outermost', 'layer', 'brain', 'cells', 'thinking', 'voluntary', 'movements', 'begin', 'cortex', 'brain', 'stem', 'spinal', 'cord', 'rest', 'brain', 'basic', 'functions', 'like', 'breathing', 'sleep', 'controlled', 'basal', 'ganglia', 'cluster', 'structures', 'center', 'brain', 'basal', 'ganglia', 'coordinate', 'messages', 'multiple', 'brain', 'areas', 'cerebellum', 'base', 'back', 'brain', 'cerebellum', 'responsible', 'coordination', 'balance', 'brain', 'also', 'divided', 'several', 'lobes', 'frontal', 'lobes', 'responsible', 'problem', 'solving', 'judgment', 'motor', 'function', 'parietal', 'lobes', 'manage', 'sensation', 'handwriting', 'body', 'position', 'temporal', 'lobes', 'involved', 'memory', 'hearing', 'occipital', 'lobes', 'contain', 'brain', 'visual', 'processing', 'system']\n",
            "stemming-----------------------------------------------------------\n",
            "-------------------------------------------------------------------\n",
            "brain  :  brain\n",
            "one  :  one\n",
            "largest  :  largest\n",
            "complex  :  complex\n",
            "organs  :  organ\n",
            "human  :  human\n",
            "body  :  bodi\n",
            "made  :  made\n",
            "billion  :  billion\n",
            "nerves  :  nerv\n",
            "communicate  :  commun\n",
            "trillions  :  trillion\n",
            "connections  :  connect\n",
            "called  :  call\n",
            "synapses  :  synaps\n",
            "brain  :  brain\n",
            "made  :  made\n",
            "many  :  mani\n",
            "specialized  :  special\n",
            "areas  :  area\n",
            "work  :  work\n",
            "together  :  togeth\n",
            "cortex  :  cortex\n",
            "outermost  :  outermost\n",
            "layer  :  layer\n",
            "brain  :  brain\n",
            "cells  :  cell\n",
            "thinking  :  think\n",
            "voluntary  :  voluntari\n",
            "movements  :  movement\n",
            "begin  :  begin\n",
            "cortex  :  cortex\n",
            "brain  :  brain\n",
            "stem  :  stem\n",
            "spinal  :  spinal\n",
            "cord  :  cord\n",
            "rest  :  rest\n",
            "brain  :  brain\n",
            "basic  :  basic\n",
            "functions  :  function\n",
            "like  :  like\n",
            "breathing  :  breath\n",
            "sleep  :  sleep\n",
            "controlled  :  control\n",
            "basal  :  basal\n",
            "ganglia  :  ganglia\n",
            "cluster  :  cluster\n",
            "structures  :  structur\n",
            "center  :  center\n",
            "brain  :  brain\n",
            "basal  :  basal\n",
            "ganglia  :  ganglia\n",
            "coordinate  :  coordin\n",
            "messages  :  messag\n",
            "multiple  :  multipl\n",
            "brain  :  brain\n",
            "areas  :  area\n",
            "cerebellum  :  cerebellum\n",
            "base  :  base\n",
            "back  :  back\n",
            "brain  :  brain\n",
            "cerebellum  :  cerebellum\n",
            "responsible  :  respons\n",
            "coordination  :  coordin\n",
            "balance  :  balanc\n",
            "brain  :  brain\n",
            "also  :  also\n",
            "divided  :  divid\n",
            "several  :  sever\n",
            "lobes  :  lobe\n",
            "frontal  :  frontal\n",
            "lobes  :  lobe\n",
            "responsible  :  respons\n",
            "problem  :  problem\n",
            "solving  :  solv\n",
            "judgment  :  judgment\n",
            "motor  :  motor\n",
            "function  :  function\n",
            "parietal  :  pariet\n",
            "lobes  :  lobe\n",
            "manage  :  manag\n",
            "sensation  :  sensat\n",
            "handwriting  :  handwrit\n",
            "body  :  bodi\n",
            "position  :  posit\n",
            "temporal  :  tempor\n",
            "lobes  :  lobe\n",
            "involved  :  involv\n",
            "memory  :  memori\n",
            "hearing  :  hear\n",
            "occipital  :  occipit\n",
            "lobes  :  lobe\n",
            "contain  :  contain\n",
            "brain  :  brain\n",
            "visual  :  visual\n",
            "processing  :  process\n",
            "system  :  system\n",
            "\n",
            "lemmatize--------------------------------------------------\n",
            "-----------------------------------------------------------\n",
            "brain ---> brain\n",
            "one ---> one\n",
            "largest ---> largest\n",
            "complex ---> complex\n",
            "organs ---> organ\n",
            "human ---> human\n",
            "body ---> body\n",
            "made ---> made\n",
            "billion ---> billion\n",
            "nerves ---> nerve\n",
            "communicate ---> communicate\n",
            "trillions ---> trillion\n",
            "connections ---> connection\n",
            "called ---> called\n",
            "synapses ---> synapsis\n",
            "brain ---> brain\n",
            "made ---> made\n",
            "many ---> many\n",
            "specialized ---> specialized\n",
            "areas ---> area\n",
            "work ---> work\n",
            "together ---> together\n",
            "cortex ---> cortex\n",
            "outermost ---> outermost\n",
            "layer ---> layer\n",
            "brain ---> brain\n",
            "cells ---> cell\n",
            "thinking ---> thinking\n",
            "voluntary ---> voluntary\n",
            "movements ---> movement\n",
            "begin ---> begin\n",
            "cortex ---> cortex\n",
            "brain ---> brain\n",
            "stem ---> stem\n",
            "spinal ---> spinal\n",
            "cord ---> cord\n",
            "rest ---> rest\n",
            "brain ---> brain\n",
            "basic ---> basic\n",
            "functions ---> function\n",
            "like ---> like\n",
            "breathing ---> breathing\n",
            "sleep ---> sleep\n",
            "controlled ---> controlled\n",
            "basal ---> basal\n",
            "ganglia ---> ganglion\n",
            "cluster ---> cluster\n",
            "structures ---> structure\n",
            "center ---> center\n",
            "brain ---> brain\n",
            "basal ---> basal\n",
            "ganglia ---> ganglion\n",
            "coordinate ---> coordinate\n",
            "messages ---> message\n",
            "multiple ---> multiple\n",
            "brain ---> brain\n",
            "areas ---> area\n",
            "cerebellum ---> cerebellum\n",
            "base ---> base\n",
            "back ---> back\n",
            "brain ---> brain\n",
            "cerebellum ---> cerebellum\n",
            "responsible ---> responsible\n",
            "coordination ---> coordination\n",
            "balance ---> balance\n",
            "brain ---> brain\n",
            "also ---> also\n",
            "divided ---> divided\n",
            "several ---> several\n",
            "lobes ---> lobe\n",
            "frontal ---> frontal\n",
            "lobes ---> lobe\n",
            "responsible ---> responsible\n",
            "problem ---> problem\n",
            "solving ---> solving\n",
            "judgment ---> judgment\n",
            "motor ---> motor\n",
            "function ---> function\n",
            "parietal ---> parietal\n",
            "lobes ---> lobe\n",
            "manage ---> manage\n",
            "sensation ---> sensation\n",
            "handwriting ---> handwriting\n",
            "body ---> body\n",
            "position ---> position\n",
            "temporal ---> temporal\n",
            "lobes ---> lobe\n",
            "involved ---> involved\n",
            "memory ---> memory\n",
            "hearing ---> hearing\n",
            "occipital ---> occipital\n",
            "lobes ---> lobe\n",
            "contain ---> contain\n",
            "brain ---> brain\n",
            "visual ---> visual\n",
            "processing ---> processing\n",
            "system ---> system\n"
          ]
        }
      ],
      "source": [
        "#Read 10 failes\n",
        "f1=open('/content/drive/MyDrive/Colab Notebooks/artificial intelligence.txt','r')\n",
        "f2=open('/content/drive/MyDrive/Colab Notebooks/Computer science.txt','r')\n",
        "f3=open('/content/drive/MyDrive/Colab Notebooks/Computer security.txt' , 'r')\n",
        "f4=open('/content/drive/MyDrive/Colab Notebooks/Cooking.txt' , 'r')\n",
        "f5=open('/content/drive/MyDrive/Colab Notebooks/Data science.txt' , 'r')\n",
        "f6=open('/content/drive/MyDrive/Colab Notebooks/Machine Learning.txt' , 'r')\n",
        "f7=open('/content/drive/MyDrive/Colab Notebooks/Natural language processing.txt' , 'r')\n",
        "f8=open('/content/drive/MyDrive/Colab Notebooks/Deep Learning.txt' ,'r')\n",
        "f9=open('/content/drive/MyDrive/Colab Notebooks/data mining.txt' , 'r')\n",
        "f10=open('/content/drive/MyDrive/Colab Notebooks/brain.txt' ,'r')\n",
        "# send to preprocessing (clean the data)\n",
        "preprocessing(f1)\n",
        "preprocessing(f2)\n",
        "preprocessing(f3)\n",
        "preprocessing(f4)\n",
        "preprocessing(f5)\n",
        "preprocessing(f6)\n",
        "preprocessing(f7)\n",
        "preprocessing(f8)\n",
        "preprocessing(f9)\n",
        "preprocessing(f10)\n",
        "f1.close()\n",
        "f2.close()\n",
        "f3.close()\n",
        "f4.close()\n",
        "f5.close()\n",
        "f6.close()\n",
        "f7.close()\n",
        "f8.close()\n",
        "f9.close()\n",
        "f10.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dOaESha-Vd85"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "yCkdoViWoxlY",
        "outputId": "88a9a487-ef05-4b97-a133-318a2c98c9c2"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-01d49f2a-9740-4803-9a9d-f03bb88a2305\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ibm</th>\n",
              "      <th>undergone</th>\n",
              "      <th>generally</th>\n",
              "      <th>work</th>\n",
              "      <th>one</th>\n",
              "      <th>served</th>\n",
              "      <th>theft</th>\n",
              "      <th>fundamental</th>\n",
              "      <th>real</th>\n",
              "      <th>pdf</th>\n",
              "      <th>...</th>\n",
              "      <th>involve</th>\n",
              "      <th>begun</th>\n",
              "      <th>hidden</th>\n",
              "      <th>data-driven</th>\n",
              "      <th>back</th>\n",
              "      <th>alan</th>\n",
              "      <th>task</th>\n",
              "      <th>ai</th>\n",
              "      <th>move</th>\n",
              "      <th>expanding</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.008403</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009009</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004022</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003522</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00885</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006186</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.009615</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.02069</td>\n",
              "      <td>0.006897</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.003606</td>\n",
              "      <td>0.002744</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.013793</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006897</td>\n",
              "      <td>0.004820</td>\n",
              "      <td>0.034483</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.012121</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.006061</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.006061</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005451</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013699</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.013699</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.005391</td>\n",
              "      <td>0.004102</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.010309</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 629 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-01d49f2a-9740-4803-9a9d-f03bb88a2305')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-01d49f2a-9740-4803-9a9d-f03bb88a2305 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-01d49f2a-9740-4803-9a9d-f03bb88a2305');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       ibm  undergone  generally      work       one    served     theft  \\\n",
              "0  0.00000   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "1  0.00000   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "2  0.00000   0.000000   0.000000  0.000000  0.000000  0.000000  0.000000   \n",
              "3  0.00000   0.000000   0.000000  0.004022  0.000000  0.000000  0.000000   \n",
              "4  0.00000   0.000000   0.000000  0.000000  0.003522  0.000000  0.000000   \n",
              "5  0.00000   0.000000   0.000000  0.000000  0.000000  0.009615  0.000000   \n",
              "6  0.02069   0.006897   0.000000  0.003606  0.002744  0.000000  0.000000   \n",
              "7  0.00000   0.000000   0.012121  0.000000  0.000000  0.000000  0.000000   \n",
              "8  0.00000   0.000000   0.000000  0.000000  0.005451  0.000000  0.013699   \n",
              "9  0.00000   0.000000   0.000000  0.005391  0.004102  0.000000  0.000000   \n",
              "\n",
              "   fundamental     real       pdf  ...   involve     begun    hidden  \\\n",
              "0     0.000000  0.00000  0.000000  ...  0.000000  0.016667  0.000000   \n",
              "1     0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.008403   \n",
              "2     0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "3     0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "4     0.000000  0.00885  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "5     0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "6     0.000000  0.00000  0.013793  ...  0.000000  0.000000  0.000000   \n",
              "7     0.006061  0.00000  0.000000  ...  0.006061  0.000000  0.000000   \n",
              "8     0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "9     0.000000  0.00000  0.000000  ...  0.000000  0.000000  0.000000   \n",
              "\n",
              "   data-driven      back      alan      task        ai      move  expanding  \n",
              "0     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
              "1     0.000000  0.000000  0.000000  0.000000  0.000000  0.008403   0.000000  \n",
              "2     0.009009  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
              "3     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
              "4     0.000000  0.000000  0.000000  0.006186  0.000000  0.000000   0.000000  \n",
              "5     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
              "6     0.000000  0.000000  0.006897  0.004820  0.034483  0.000000   0.000000  \n",
              "7     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
              "8     0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   0.013699  \n",
              "9     0.000000  0.010309  0.000000  0.000000  0.000000  0.000000   0.000000  \n",
              "\n",
              "[10 rows x 629 columns]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "docA = open('/content/drive/MyDrive/Colab Notebooks/DL_clean.txt','r')\n",
        "docB = open('/content/drive/MyDrive/Colab Notebooks/DM_clean.txt','r')\n",
        "docC = open('/content/drive/MyDrive/Colab Notebooks/DS_clean.txt','r')\n",
        "docD = open('/content/drive/MyDrive/Colab Notebooks/MS_clean.txt','r')\n",
        "docE = open('/content/drive/MyDrive/Colab Notebooks/NLP_clean.txt','r')\n",
        "docF = open('/content/drive/MyDrive/Colab Notebooks/cook_clean.txt','r')\n",
        "docG = open('/content/drive/MyDrive/Colab Notebooks/AI_clean.txt','r')\n",
        "docH = open('/content/drive/MyDrive/Colab Notebooks/CS_clean.txt','r')\n",
        "docI = open('/content/drive/MyDrive/Colab Notebooks/CSec_clean.txt','r')\n",
        "docJ =open('/content/drive/MyDrive/Colab Notebooks/brain_clean.txt','r')\n",
        "\n",
        "#This code will split all the words in the file\n",
        "with docA as file:\n",
        "  content=file.read()\n",
        "  bowA = word_tokenize(content)\n",
        "\n",
        "with docB as file:\n",
        " content=file.read()\n",
        " bowB = word_tokenize(content)\n",
        "\n",
        "with docC as file:\n",
        " content=file.read()\n",
        " bowC = word_tokenize(content)\n",
        "\n",
        "with docD as file:\n",
        " content=file.read()\n",
        " bowD = word_tokenize(content)\n",
        "\n",
        "with docE as file:\n",
        " content=file.read()\n",
        " bowE = word_tokenize(content)\n",
        "\n",
        "with docF as file:\n",
        " content=file.read()\n",
        " bowF = word_tokenize(content)\n",
        "\n",
        "with docG as file:\n",
        " content=file.read()\n",
        " bowG = word_tokenize(content)\n",
        "\n",
        "with docH as file:\n",
        " content=file.read()\n",
        " bowH = word_tokenize(content)\n",
        "\n",
        "with docI as file:\n",
        " content=file.read()\n",
        " bowI = word_tokenize(content)\n",
        "\n",
        "with docJ as file:\n",
        " content=file.read()\n",
        " bowJ = word_tokenize(content)\n",
        "\n",
        "#This code will remove duplicate words in all files\n",
        "wordSet = set(bowA).union(set(bowB)).union(set(bowC)).union(set(bowD)).union(set(bowE)).union(set(bowF)).union(set(bowG)).union(set(bowH)).union(set(bowI)).union(set(bowJ))\n",
        "\n",
        "\n",
        "wordDictA = dict.fromkeys(wordSet, 0)\n",
        "wordDictB = dict.fromkeys(wordSet, 0)\n",
        "wordDictC = dict.fromkeys(wordSet, 0)\n",
        "wordDictD = dict.fromkeys(wordSet, 0)\n",
        "wordDictE = dict.fromkeys(wordSet, 0)\n",
        "wordDictF = dict.fromkeys(wordSet, 0)\n",
        "wordDictG = dict.fromkeys(wordSet, 0)\n",
        "wordDictH = dict.fromkeys(wordSet, 0)\n",
        "wordDictI = dict.fromkeys(wordSet, 0)\n",
        "wordDictJ = dict.fromkeys(wordSet, 0)\n",
        "\n",
        "\n",
        "#I will calculate the frequency of the word in each file\n",
        "for word in bowA:\n",
        "  wordDictA[word]+=1\n",
        "\n",
        "for word in bowB:\n",
        " wordDictB[word]+=1\n",
        "\n",
        "for word in bowC:\n",
        " wordDictC[word]+=1\n",
        "\n",
        "for word in bowD:\n",
        " wordDictD[word]+=1\n",
        "\n",
        "for word in bowE:\n",
        " wordDictE[word]+=1\n",
        "\n",
        "for word in bowF:\n",
        "  wordDictF[word]+=1\n",
        "\n",
        "for word in bowG:\n",
        "  wordDictG[word]+=1\n",
        "\n",
        "for word in bowH:\n",
        "  wordDictH[word]+=1\n",
        "\n",
        "for word in bowI:\n",
        "  wordDictI[word]+=1\n",
        "\n",
        "for word in bowJ:\n",
        "  wordDictJ[word]+=1\n",
        "\n",
        "#This code will calculate TF\n",
        "def computeTF(wordDict, bow):\n",
        "    tfDict = {}\n",
        "    bowCount = len(bow)\n",
        "    for word, count in wordDict.items():\n",
        "        tfDict[word] = count/float(bowCount)\n",
        "    return tfDict\n",
        "\n",
        "tfBowA= computeTF(wordDictA, bowA)\n",
        "tfBowB= computeTF(wordDictB, bowB)\n",
        "tfBowC= computeTF(wordDictC, bowC)\n",
        "tfBowD= computeTF(wordDictD, bowD)\n",
        "tfBowE= computeTF(wordDictE, bowE)\n",
        "tfBowF= computeTF(wordDictF, bowF)\n",
        "tfBowG= computeTF(wordDictG, bowG)\n",
        "tfBowH= computeTF(wordDictH, bowH)\n",
        "tfBowI= computeTF(wordDictI, bowI)\n",
        "tfBowJ= computeTF(wordDictJ, bowJ)\n",
        "\n",
        "#This code will calculate the IDF\n",
        "def computeIDF(docList):\n",
        "    import math\n",
        "\n",
        "    idfDict = {}\n",
        "    N = len(docList)\n",
        "\n",
        "    idfDict = dict.fromkeys(docList[0].keys(), 0)\n",
        "    for doc in docList:\n",
        "       for word, val in doc.items():\n",
        "          if val > 0:\n",
        "             idfDict[word] += 1\n",
        "\n",
        "    for word, val in idfDict.items():\n",
        "        idfDict[word] = math.log10(N/float(val))\n",
        "\n",
        "    return idfDict\n",
        "\n",
        "#I'll put it on the list\n",
        "idfs = computeIDF([wordDictA, wordDictB,wordDictC,wordDictD,wordDictE,wordDictF,wordDictG,wordDictH,wordDictI,wordDictJ])\n",
        "\n",
        "#This code will calculate TF-IDF\n",
        "def computeTFIDF(tfBow, idfs):\n",
        " tfidf = {}\n",
        " for word, val in tfBow.items():\n",
        "    tfidf[word] = val*idfs[word]\n",
        " return tfidf\n",
        "\n",
        "\n",
        "tfidfBowA = computeTFIDF(tfBowA, idfs)\n",
        "tfidfBowB = computeTFIDF(tfBowB, idfs)\n",
        "tfidfBowC = computeTFIDF(tfBowC, idfs)\n",
        "tfidfBowD = computeTFIDF(tfBowD, idfs)\n",
        "tfidfBowE = computeTFIDF(tfBowE, idfs)\n",
        "tfidfBowF = computeTFIDF(tfBowF, idfs)\n",
        "tfidfBowG = computeTFIDF(tfBowG, idfs)\n",
        "tfidfBowH = computeTFIDF(tfBowH, idfs)\n",
        "tfidfBowI = computeTFIDF(tfBowI, idfs)\n",
        "tfidfBowJ = computeTFIDF(tfBowJ, idfs)\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame([tfidfBowA, tfidfBowB,tfidfBowC,tfidfBowD,tfidfBowE,tfidfBowF,tfidfBowG,tfidfBowH,tfidfBowI,tfidfBowJ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CM8VI3E9k51b",
        "outputId": "519d7e20-c831-42f0-c5af-82978da17823"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "import gensim\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uIakolflx7VE",
        "outputId": "ce645d4a-8f9e-48a9-d68a-504844d85cef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your query: computer\n",
            "the similarity bewteen documents and the query  is : \n",
            "document Artificial intelligence : 0.20811957\n",
            "document Computer Science : 0.08096948\n",
            "document computer Security : 0.08075692\n",
            "document Data Science : 0.07980711\n",
            "document Machine Learning : 0.045335367\n",
            "document NLP : 0.040058173\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gensim/similarities/docsim.py:518: FutureWarning: arrays to stack must be passed as a \"sequence\" type such as list or tuple. Support for non-sequence iterables such as generators is deprecated as of NumPy 1.16 and will raise an error in the future.\n",
            "  result = numpy.hstack(shard_results)\n"
          ]
        }
      ],
      "source": [
        "#Open file and tokenize sentences\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "file_docs = []\n",
        "\n",
        "with open ('demofile.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file_docs.append(line)\n",
        "\n",
        "#Tokenize words and create dictionary\n",
        "gen_docs = [[w.lower() for w in word_tokenize(text)]\n",
        "            for text in file_docs]\n",
        "\n",
        "dictionary = gensim.corpora.Dictionary(gen_docs)\n",
        "\n",
        "\n",
        "#Create a bag of words\n",
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]\n",
        "\n",
        "\n",
        "#TFIDF\n",
        "# words that occur more frequently across the documents get smaller weights.\n",
        "tf_idf = gensim.models.TfidfModel(corpus)\n",
        "\n",
        "\n",
        "#Creating similarity measure object\n",
        "# building the index\n",
        "sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],\n",
        "                                    num_features=len(dictionary))\n",
        "\n",
        "#Create Query Document\n",
        "file2_docs = []\n",
        "list1=['Artificial intelligence','brain','cook','Computer Science','computer Security','Deep Learining ','Data Mining','Data Science','Machine Learning','NLP']\n",
        "list2=[]\n",
        "q=input('Enter your query: ')\n",
        "\n",
        "with open ('demofile2.txt', 'w') as r:\n",
        "  r.write(q)\n",
        "with open ('demofile2.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "\n",
        "for line in file2_docs:\n",
        "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "    # update an existing dictionary and create bag of word\n",
        "    query_doc_bow = dictionary.doc2bow(query_doc)\n",
        "\n",
        "print('the similarity bewteen documents and the query  is : ')\n",
        "\n",
        "# perform a similarity query against the corpus\n",
        "query_doc_tf_idf = tf_idf[query_doc_bow]\n",
        "n=0\n",
        "s=0\n",
        "sum=0\n",
        "for item in sims[query_doc_tf_idf] :\n",
        "  if item > 0.00000000 :\n",
        "    list2.append(s)\n",
        "  s=s+1\n",
        "for item in sorted(sims[query_doc_tf_idf],reverse = True):\n",
        "\n",
        "   if item > 0.00000000 :\n",
        "    #list2.append(n)\n",
        "    print(\"document\",list1[list2[n]],\":\",item)\n",
        "   n=n+1\n",
        "   sum=item+sum\n",
        "if sum==0:\n",
        "  print('it is not found ')\n",
        "  print()\n",
        "\n",
        "\n",
        ""
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}